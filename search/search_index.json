{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to REALM_AI's documentation!","title":"Welcome to REALM_AI's documentation!"},{"location":"#welcome-to-realm_ais-documentation","text":"","title":"Welcome to REALM_AI's documentation!"},{"location":"installation/","text":"Quick Start Guide Brief Overview of Installation To have the entire pipeline working, there are 5 main components to install: Unity Game Plugin RL Subsystem ( realm-tune ) Python GUI ( realm-gui ) Report Subsystem ( realm-report ) The installation process for each of them are very straightforward, and are to be individually installed to facilitate flexibility to a user's needs. For a more formal description of each of these subsystems, please refer them to their individual sections in this documentation. Installation Steps Installing ML-Agents The REALM-AI tools are tightly integrated with Unity ML-Agents. As a result, it would be advisable to have Unity ML-Agents installed first, to ensure that installation of the following REALM-AI packages go as smoothly as possible. To install Unity ML-Agents, please refer to their excellent installation guide . Unity Game Plugin Since the installation process for the Unity Game Plugin is slightly more involved, we refer users to the installation guide provided as part of its documentation, here . RL Subsystem ( realm-tune ) To install the various tools bundled with the RL Subsystem, perform the following steps in a terminal: git clone https://github.com/realm-ai-project/RL-Subsystem.git cd RL-Subsystem pip install -e . Python GUI ( realm-gui ) To install the Python GUI, which is a custom-built UI that is tightly knitted with the Unity Game Plugin, perform the following steps in a terminal: git clone https://github.com/realm-ai-project/Python-GUI.git cd Python-GUI pip install -e . Report Subsystem ( realm-report ) The Report Subsystem is composed of a React frontend, and a Flask backend. These two components work closely with each other in order to generate and display heatmaps to the user. To install the realm-report tool, perform the following steps: git clone https://github.com/realm-ai-project/Reporting-Subsystem.git cd Reporting-Subsystem/api pip install -e .","title":"Quick Start Guide"},{"location":"installation/#quick-start-guide","text":"","title":"Quick Start Guide"},{"location":"installation/#brief-overview-of-installation","text":"To have the entire pipeline working, there are 5 main components to install: Unity Game Plugin RL Subsystem ( realm-tune ) Python GUI ( realm-gui ) Report Subsystem ( realm-report ) The installation process for each of them are very straightforward, and are to be individually installed to facilitate flexibility to a user's needs. For a more formal description of each of these subsystems, please refer them to their individual sections in this documentation.","title":"Brief Overview of Installation"},{"location":"installation/#installation-steps","text":"","title":"Installation Steps"},{"location":"installation/#installing-ml-agents","text":"The REALM-AI tools are tightly integrated with Unity ML-Agents. As a result, it would be advisable to have Unity ML-Agents installed first, to ensure that installation of the following REALM-AI packages go as smoothly as possible. To install Unity ML-Agents, please refer to their excellent installation guide .","title":"Installing ML-Agents"},{"location":"installation/#unity-game-plugin","text":"Since the installation process for the Unity Game Plugin is slightly more involved, we refer users to the installation guide provided as part of its documentation, here .","title":"Unity Game Plugin"},{"location":"installation/#rl-subsystem-realm-tune","text":"To install the various tools bundled with the RL Subsystem, perform the following steps in a terminal: git clone https://github.com/realm-ai-project/RL-Subsystem.git cd RL-Subsystem pip install -e .","title":"RL Subsystem (realm-tune)"},{"location":"installation/#python-gui-realm-gui","text":"To install the Python GUI, which is a custom-built UI that is tightly knitted with the Unity Game Plugin, perform the following steps in a terminal: git clone https://github.com/realm-ai-project/Python-GUI.git cd Python-GUI pip install -e .","title":"Python GUI (realm-gui)"},{"location":"installation/#report-subsystem-realm-report","text":"The Report Subsystem is composed of a React frontend, and a Flask backend. These two components work closely with each other in order to generate and display heatmaps to the user. To install the realm-report tool, perform the following steps: git clone https://github.com/realm-ai-project/Reporting-Subsystem.git cd Reporting-Subsystem/api pip install -e .","title":"Report Subsystem (realm-report)"},{"location":"analysis_subsystem/description/","text":"The main goal of the Analysis subsystem is to automatically create different heatmaps based on the data generated upstream of the pipeline.","title":"Description"},{"location":"analysis_subsystem/heatmaps/","text":"What Are Heatmaps? Hello world Heatmap 1 - Generic Heatmap I wanna eat Heatmap 2 - I wanna sleep Heatmap 3 - I wanna cry Heatmap 4 - I wanna die","title":"Heatmaps"},{"location":"analysis_subsystem/heatmaps/#what-are-heatmaps","text":"Hello world","title":"What Are Heatmaps?"},{"location":"analysis_subsystem/heatmaps/#heatmap-1-generic-heatmap","text":"I wanna eat","title":"Heatmap 1 - Generic Heatmap"},{"location":"analysis_subsystem/heatmaps/#heatmap-2-","text":"I wanna sleep","title":"Heatmap 2 -"},{"location":"analysis_subsystem/heatmaps/#heatmap-3-","text":"I wanna cry","title":"Heatmap 3 -"},{"location":"analysis_subsystem/heatmaps/#heatmap-4-","text":"I wanna die","title":"Heatmap 4 -"},{"location":"feature_request/todo/","text":"Game Subsystem If memory becomes a problem due to size of .dat file, adjust logging frequency Reporting Subsystem Embed TensorBoard as an IFrame ( example ) within the reporting subsystem so that users do not need to go to a different website Range slider: the idea is to allow users to select the timeframe window of which they would like the heatmap to be plotted over Example: Screenshot from Plotly Related to the range slider, but a more conventional slider that allows users to view captured videos/screenshots on a timeline Example: Screenshot from Weights & Biases For the data storage file format, consider creating an EBML based format, which combines the space-efficiency of pure binary formats and the flexbility/forwards+backwards compatibility of json. The main drawbacks is that it is not easily humanly readable and may be more complex to read and write. Notable existing formats based on EBML are WEBM and MKV.","title":"Feature Request"},{"location":"feature_request/todo/#game-subsystem","text":"If memory becomes a problem due to size of .dat file, adjust logging frequency","title":"Game Subsystem"},{"location":"feature_request/todo/#reporting-subsystem","text":"Embed TensorBoard as an IFrame ( example ) within the reporting subsystem so that users do not need to go to a different website Range slider: the idea is to allow users to select the timeframe window of which they would like the heatmap to be plotted over Example: Screenshot from Plotly Related to the range slider, but a more conventional slider that allows users to view captured videos/screenshots on a timeline Example: Screenshot from Weights & Biases For the data storage file format, consider creating an EBML based format, which combines the space-efficiency of pure binary formats and the flexbility/forwards+backwards compatibility of json. The main drawbacks is that it is not easily humanly readable and may be more complex to read and write. Notable existing formats based on EBML are WEBM and MKV.","title":"Reporting Subsystem"},{"location":"game_plugin_subsystem/installation/","text":"Installation The installation process for the users is detailed in the image below, which should also provide an overview of how the package is used. For step 4, the configuration required of the user is detailed in the Prefab Breakdown page. The Unity package is designed to be installed simply from its git URL using Unity's built-in package management system. The package is also designed to be able to be installed into an existing game project. Should there be conflicts with other installed packages, this documentation currently does not provide any details on resolving potential conflicts with other packages. Installation for Development For development, instead of using the \"Add package from git URL...\" option, clone the repo separately and use the \"Add package from disk...\" option. In the dialog popup, select the package.json file from the cloned repo. This will install directly from the cloned repo. You can then directly modify the package from inside Unity and the changes will be made directly to the files in the cloned repo.","title":"Installation"},{"location":"game_plugin_subsystem/installation/#installation","text":"The installation process for the users is detailed in the image below, which should also provide an overview of how the package is used. For step 4, the configuration required of the user is detailed in the Prefab Breakdown page. The Unity package is designed to be installed simply from its git URL using Unity's built-in package management system. The package is also designed to be able to be installed into an existing game project. Should there be conflicts with other installed packages, this documentation currently does not provide any details on resolving potential conflicts with other packages.","title":"Installation"},{"location":"game_plugin_subsystem/installation/#installation-for-development","text":"For development, instead of using the \"Add package from git URL...\" option, clone the repo separately and use the \"Add package from disk...\" option. In the dialog popup, select the package.json file from the cloned repo. This will install directly from the cloned repo. You can then directly modify the package from inside Unity and the changes will be made directly to the files in the cloned repo.","title":"Installation for Development"},{"location":"game_plugin_subsystem/installation_configuration/","text":"Configuring the Realm AI Module All of the configuration needed for RealmAI features are stored within the instance of Realm AI Module (which should be a child of the player prefab). There are a number of things to configure in the Realm AI Module before the RealmAI features will work. To help with this, all of the essential configuration and inspector fields that need to be filled will be shown (mirrored) in the Configuration Window itself, so you don't have to search through the Realm AI Module to find them. These inspectors fields can be configured, the ones that are not optional must be filled before the RealmAI features will work: Name Location Required Description Initialize Function Realm AI Module: Realm Agent Yes Provide a function here to intiailize the game environment. This will be called once at the start of the application (assuming this script is active) Reset Function Realm AI Module: Realm Agent Yes Provide a function here to reset the game environment. This will be called once at the beginning of each episode (we will consider each run of the game to be one episode). Game Over Function Realm AI Module: Realm Agent Yes Provide a function which will return a bool that is true when the game is over. This will be called every frame to see if the current episode has ended. Max Episode Length (Episode Timeout) Realm AI Module: Realm Agent Yes If set to a positive number, the current episode will end after this many seconds. It is recommended to set this to a value large enough for the player to complete what you want them to do, but not too large that a clueless bot can get stuck somewhere for a long time without the game being restarted. Note that you can also include a time factor into the Game Over Function to end the game if the player has not done anything significant for a while. Position Function Realm AI Module/Sensor: Realm Sensor Component Yes Provide a function that returns the current position of the player. This will be called frequently to record the players position, and to update the bot with the player's location Approximate Map Bounds Realm AI Module/Sensor: Realm Sensor Component No Describe the approximate bounaries of the player. If you set this properly, it can help the bot learn faster. (Grid) Cell Size 1 Realm AI Module/Grid Sensor 2D Component No 1 The size of each cell in the grid sensor (Grid) Cell Count 1 Realm AI Module/Grid Sensor 2D Component No 1 The number of cells in the grid sensor along each axis Detectable Tags 1 Realm AI Module/Grid Sensor 2D Component No 1 The tags of the objects that should be detectable by the grid sensor Collider Mask 1 Realm AI Module/Grid Sensor 2D Component No 1 The layers to scan for the colliders of detectable objects. Show (Grid) Gizmos 1 Realm AI Module/Grid Sensor 2D Component No 1 Visualize the grid and its detections with Gizmos. (Grid) Gizmos Colors 1 Realm AI Module/Grid Sensor 2D Component No 1 Assign the color (one corresponding to each detectable tag) for detected objects to show up as in the grid Gizmos. Custom Sensors Realm AI Module/Sensor: Realm Sensor Component No If there are any internal states not detected by the Grid Sensor, add them here so the bot will know about them, potentially improving its performance. For example, cooldowns, amount of health, and other variable stats can be useful for the bots decision making. If it helps, you can imagine the bot having absolutely no memory, and can only make decisions from its position, the grid sensor, and any additional observations you provide here. Show (Sensor) Gizmos 1 Realm AI Module/Realm Sensor Component No 1 Display the values received by the custom sensors to verify they are working. Note that values will be converted to floats and integers will be normalized between their ranges. Position Record Interval Realm AI Module/Sensor: Realm Sensor Component No This number is the interval for how often the position is recorded for the heatmap. Set this to X to record the player's position every X seconds. Actions Realm AI Module/Actuator: Realm Actuator Component Yes Configure this to specify the actions that the player can take. The bot will use this to control the player. For each action, specify a Callback, which is a setter function the bot can use to set the value for the action. Optionally, specify a Heuristic, which is a getter function that returns current human's input value for the action. Essentially, when the bot isn't playing, the Heuristic function can be used to get human input, which we will in turn pass into the Callback function to control the player. Reward Function Realm AI Module/Score: Realm Score Yes 2 Provide a function that the current score of the player. The bot will learn to maximize its score during training. Reward Regions Realm AI Module/Score: Realm Score No 2 Define regions in the world where the player should be rewarded for reaching. Scores awared or deducted from the player entering or staying in these regions are added on top of the Reward Function Existential Penalty (Per Second) Realm AI Module/Score: Realm Score No 2 Add a score penalty to the bot for existing. This amount will be deducted from the bots score at a constant rate per second. Adding a small existential penalty to the bot can encourage it to complete each run of the game faster. Show (Reward) Gizmos Realm AI Module/Score: Realm Score No Show the current score achieved by the bot using Gizmos. Video Resolution 3 Realm AI Module/VideoRecording: Realm Recorder No The resolution of video replays. Video Per Million Steps 3 Realm AI Module/VideoRecording: Realm Recorder No The amount of videos to record per 1 million steps of training. Feel free to experiment with different amounts of videos recorded to suit your needs. 1 The Grid Sensor is the main method for the bot to perceive the environment. Configuring this sensor is highly recommended, although it is optional if the other sensors provide enough information for the bot to play well . It works by detecting objects in a grid around the player. Objects with a 2D collider are detected when their collider touches a cell, as long as their tag and layer are specified. Only one object is detectable per cell, and the order of priority of detection is specified by the order in with they appear in the Detectable Tags array (the first elements in the array has priority over later elements). 2 Providing a reward function is technically not necessary, as long as there is some form of reward for the bot the learn from. At least one of these reward methods should be set, but a custom reward function (which can simply be an existing score function you already have) is the most common way to do it. 3 As an optional feature, video replays can be recorded. They are intended to be recorded periodically during training, to allow you to see the bot play and to reveal any interesting behavior not visible from the heatmaps. To enable video replays, provide a path to a FFmpeg executable on your machine in the FFmpeg Path setting in the Realm AI Configuration Window. Note that video recording will not work if you are running a build in no-graphics mode.","title":"Configuring the Realm AI Module"},{"location":"game_plugin_subsystem/installation_configuration/#configuring-the-realm-ai-module","text":"All of the configuration needed for RealmAI features are stored within the instance of Realm AI Module (which should be a child of the player prefab). There are a number of things to configure in the Realm AI Module before the RealmAI features will work. To help with this, all of the essential configuration and inspector fields that need to be filled will be shown (mirrored) in the Configuration Window itself, so you don't have to search through the Realm AI Module to find them. These inspectors fields can be configured, the ones that are not optional must be filled before the RealmAI features will work: Name Location Required Description Initialize Function Realm AI Module: Realm Agent Yes Provide a function here to intiailize the game environment. This will be called once at the start of the application (assuming this script is active) Reset Function Realm AI Module: Realm Agent Yes Provide a function here to reset the game environment. This will be called once at the beginning of each episode (we will consider each run of the game to be one episode). Game Over Function Realm AI Module: Realm Agent Yes Provide a function which will return a bool that is true when the game is over. This will be called every frame to see if the current episode has ended. Max Episode Length (Episode Timeout) Realm AI Module: Realm Agent Yes If set to a positive number, the current episode will end after this many seconds. It is recommended to set this to a value large enough for the player to complete what you want them to do, but not too large that a clueless bot can get stuck somewhere for a long time without the game being restarted. Note that you can also include a time factor into the Game Over Function to end the game if the player has not done anything significant for a while. Position Function Realm AI Module/Sensor: Realm Sensor Component Yes Provide a function that returns the current position of the player. This will be called frequently to record the players position, and to update the bot with the player's location Approximate Map Bounds Realm AI Module/Sensor: Realm Sensor Component No Describe the approximate bounaries of the player. If you set this properly, it can help the bot learn faster. (Grid) Cell Size 1 Realm AI Module/Grid Sensor 2D Component No 1 The size of each cell in the grid sensor (Grid) Cell Count 1 Realm AI Module/Grid Sensor 2D Component No 1 The number of cells in the grid sensor along each axis Detectable Tags 1 Realm AI Module/Grid Sensor 2D Component No 1 The tags of the objects that should be detectable by the grid sensor Collider Mask 1 Realm AI Module/Grid Sensor 2D Component No 1 The layers to scan for the colliders of detectable objects. Show (Grid) Gizmos 1 Realm AI Module/Grid Sensor 2D Component No 1 Visualize the grid and its detections with Gizmos. (Grid) Gizmos Colors 1 Realm AI Module/Grid Sensor 2D Component No 1 Assign the color (one corresponding to each detectable tag) for detected objects to show up as in the grid Gizmos. Custom Sensors Realm AI Module/Sensor: Realm Sensor Component No If there are any internal states not detected by the Grid Sensor, add them here so the bot will know about them, potentially improving its performance. For example, cooldowns, amount of health, and other variable stats can be useful for the bots decision making. If it helps, you can imagine the bot having absolutely no memory, and can only make decisions from its position, the grid sensor, and any additional observations you provide here. Show (Sensor) Gizmos 1 Realm AI Module/Realm Sensor Component No 1 Display the values received by the custom sensors to verify they are working. Note that values will be converted to floats and integers will be normalized between their ranges. Position Record Interval Realm AI Module/Sensor: Realm Sensor Component No This number is the interval for how often the position is recorded for the heatmap. Set this to X to record the player's position every X seconds. Actions Realm AI Module/Actuator: Realm Actuator Component Yes Configure this to specify the actions that the player can take. The bot will use this to control the player. For each action, specify a Callback, which is a setter function the bot can use to set the value for the action. Optionally, specify a Heuristic, which is a getter function that returns current human's input value for the action. Essentially, when the bot isn't playing, the Heuristic function can be used to get human input, which we will in turn pass into the Callback function to control the player. Reward Function Realm AI Module/Score: Realm Score Yes 2 Provide a function that the current score of the player. The bot will learn to maximize its score during training. Reward Regions Realm AI Module/Score: Realm Score No 2 Define regions in the world where the player should be rewarded for reaching. Scores awared or deducted from the player entering or staying in these regions are added on top of the Reward Function Existential Penalty (Per Second) Realm AI Module/Score: Realm Score No 2 Add a score penalty to the bot for existing. This amount will be deducted from the bots score at a constant rate per second. Adding a small existential penalty to the bot can encourage it to complete each run of the game faster. Show (Reward) Gizmos Realm AI Module/Score: Realm Score No Show the current score achieved by the bot using Gizmos. Video Resolution 3 Realm AI Module/VideoRecording: Realm Recorder No The resolution of video replays. Video Per Million Steps 3 Realm AI Module/VideoRecording: Realm Recorder No The amount of videos to record per 1 million steps of training. Feel free to experiment with different amounts of videos recorded to suit your needs. 1 The Grid Sensor is the main method for the bot to perceive the environment. Configuring this sensor is highly recommended, although it is optional if the other sensors provide enough information for the bot to play well . It works by detecting objects in a grid around the player. Objects with a 2D collider are detected when their collider touches a cell, as long as their tag and layer are specified. Only one object is detectable per cell, and the order of priority of detection is specified by the order in with they appear in the Detectable Tags array (the first elements in the array has priority over later elements). 2 Providing a reward function is technically not necessary, as long as there is some form of reward for the bot the learn from. At least one of these reward methods should be set, but a custom reward function (which can simply be an existing score function you already have) is the most common way to do it. 3 As an optional feature, video replays can be recorded. They are intended to be recorded periodically during training, to allow you to see the bot play and to reveal any interesting behavior not visible from the heatmaps. To enable video replays, provide a path to a FFmpeg executable on your machine in the FFmpeg Path setting in the Realm AI Configuration Window. Note that video recording will not work if you are running a build in no-graphics mode.","title":"Configuring the Realm AI Module"},{"location":"game_plugin_subsystem/installation_player/","text":"Project Requirements, Setting up the Player Prefab To setup RealmAI, it requires there to be a prefab representing the player in the game project. This prefab should: - represent the player, i.e. its position represents the player's position in the game world - be present in the game Scene or be spawned in as soon as the application is run - have only one instance in the game environment throughout the game - ensure that its one instance is never deactivated, disabled, or destroyed throughout the game If your project does not have a player prefab that satisfy these requirements, please create one, or adjust your code so that the requirements are satisfied. If these requirements cannot be satisfied, your project may not be suitable for RealmAI \ud83d\ude22. Once you have a player prefab that satisfies the requirements above, let RealmAI know about this prefab by assigning the prefab to the \"Player Prefab\" field in the Configuration Window in Unity (found under Realm AI/Open Configuration Window), which should look like this: To continue, open click \"Open player prefab for edit\" to open the player prefab in prefab edit mode. Once in prefab edit mode, click on \"Add Realm AI Module\" to add an instance of the \"Realm AI Module\" prefab (included with the RealmAI package) as a child of the player prefab. This Realm AI Module contains a series of scripts for RealmAI features, which also store all of the configuration needed for these features. Note: like with the player, there should only be one active instance of the Realm AI Module throughout the game. If you have prefab variants that derive from the player prefab, please disable the Realm AI Module in those prefabs variants.","title":"Project Requirements, Setting up the Player Prefab"},{"location":"game_plugin_subsystem/installation_player/#project-requirements-setting-up-the-player-prefab","text":"To setup RealmAI, it requires there to be a prefab representing the player in the game project. This prefab should: - represent the player, i.e. its position represents the player's position in the game world - be present in the game Scene or be spawned in as soon as the application is run - have only one instance in the game environment throughout the game - ensure that its one instance is never deactivated, disabled, or destroyed throughout the game If your project does not have a player prefab that satisfy these requirements, please create one, or adjust your code so that the requirements are satisfied. If these requirements cannot be satisfied, your project may not be suitable for RealmAI \ud83d\ude22. Once you have a player prefab that satisfies the requirements above, let RealmAI know about this prefab by assigning the prefab to the \"Player Prefab\" field in the Configuration Window in Unity (found under Realm AI/Open Configuration Window), which should look like this: To continue, open click \"Open player prefab for edit\" to open the player prefab in prefab edit mode. Once in prefab edit mode, click on \"Add Realm AI Module\" to add an instance of the \"Realm AI Module\" prefab (included with the RealmAI package) as a child of the player prefab. This Realm AI Module contains a series of scripts for RealmAI features, which also store all of the configuration needed for these features. Note: like with the player, there should only be one active instance of the Realm AI Module throughout the game. If you have prefab variants that derive from the player prefab, please disable the Realm AI Module in those prefabs variants.","title":"Project Requirements, Setting up the Player Prefab"},{"location":"game_plugin_subsystem/installation_python/","text":"Setting up the Python Environment The Unity Package requires the system to have Python 3 installed (this package is tested with version 3.7, but other versions may work), and to have the Python packages realm-tune , realm-gui , and realm-report installed. You can verify that these are installed by opening a command line and entering the commands realm-tune , realm-gui , or realm-report . They should all run and do something if installed correctly. Setting up the Python Environment It is often recommended to use virtual or isolated environments when using Python. This documentation will not explain why or how to do this, but if you have installed the Python dependencies into a specific environment, Realm AI needs to be able to activate the environment to use these dependencies. To set this up, open the Configuration Window in Unity (found under Realm AI/Open Configuration Window) and near the bottom, enter the command(s) to activate the environment in the \"Environment Setup Command\" field. An example of the setup for a Conda environment named \"ml\", on Windows, is shown below: If you use Python without any environments, just leave this field blank.","title":"Setting up the Python Environment"},{"location":"game_plugin_subsystem/installation_python/#setting-up-the-python-environment","text":"The Unity Package requires the system to have Python 3 installed (this package is tested with version 3.7, but other versions may work), and to have the Python packages realm-tune , realm-gui , and realm-report installed. You can verify that these are installed by opening a command line and entering the commands realm-tune , realm-gui , or realm-report . They should all run and do something if installed correctly.","title":"Setting up the Python Environment"},{"location":"game_plugin_subsystem/installation_python/#setting-up-the-python-environment_1","text":"It is often recommended to use virtual or isolated environments when using Python. This documentation will not explain why or how to do this, but if you have installed the Python dependencies into a specific environment, Realm AI needs to be able to activate the environment to use these dependencies. To set this up, open the Configuration Window in Unity (found under Realm AI/Open Configuration Window) and near the bottom, enter the command(s) to activate the environment in the \"Environment Setup Command\" field. An example of the setup for a Conda environment named \"ml\", on Windows, is shown below: If you use Python without any environments, just leave this field blank.","title":"Setting up the Python Environment"},{"location":"game_plugin_subsystem/installation_tips/","text":"Final Installation Tips and Next Steps When you are done with configuration, run the game to test out the integration. You should be able to control your character as normal and if you turn on Gizmos for the sensors and reward components, you should be able to see values for the sensors and the score achieved, as well as the objects being detected by the grid sensor. The game should also be initialized properly and should reset when the game is over. If anything is not working here, stop and revisit those settings again to debug and fix them. Once your game works, you can test out the training process in the editor. Open the Realm AI menu and select \"Train in Editor\". This will open up the Python GUI to initialize training. The options within the Python GUI is explained in the Python GUI documentation pages . When you choose \"Train in Editor\", a small editor window will open which will automatically run the game when the training starts. If the training seems to work in the editor, with no errors or bugs when you train for a while, you can perform the full training process with a build by opening the Realm AI menu and selecting \"Make Build and Train\". This will prompt you to make build, and then open up the Python GUI to initialize the full tuning and training process with the build that was just made. The \"Train with Existing Build\" option does the same thing, but instead of making a new build, you will be prompted to select an existing build to train with. When the training is done, you can come back into this menu and select \"Open Results Directory\" to open the directory in the training results are stored. You can also then select \"Open Dashboard\" to open the dashboard web app to generate heatmaps and see the list of videos replays.","title":"Final Tips and Instructions"},{"location":"game_plugin_subsystem/installation_tips/#final-installation-tips-and-next-steps","text":"When you are done with configuration, run the game to test out the integration. You should be able to control your character as normal and if you turn on Gizmos for the sensors and reward components, you should be able to see values for the sensors and the score achieved, as well as the objects being detected by the grid sensor. The game should also be initialized properly and should reset when the game is over. If anything is not working here, stop and revisit those settings again to debug and fix them. Once your game works, you can test out the training process in the editor. Open the Realm AI menu and select \"Train in Editor\". This will open up the Python GUI to initialize training. The options within the Python GUI is explained in the Python GUI documentation pages . When you choose \"Train in Editor\", a small editor window will open which will automatically run the game when the training starts. If the training seems to work in the editor, with no errors or bugs when you train for a while, you can perform the full training process with a build by opening the Realm AI menu and selecting \"Make Build and Train\". This will prompt you to make build, and then open up the Python GUI to initialize the full tuning and training process with the build that was just made. The \"Train with Existing Build\" option does the same thing, but instead of making a new build, you will be prompted to select an existing build to train with. When the training is done, you can come back into this menu and select \"Open Results Directory\" to open the directory in the training results are stored. You can also then select \"Open Dashboard\" to open the dashboard web app to generate heatmaps and see the list of videos replays.","title":"Final Installation Tips and Next Steps"},{"location":"game_plugin_subsystem/installation_unity/","text":"Installing the Unity Package To install the Unity Package, use the Unity Package Manager to install it from the GitHub repository https://github.com/realm-ai-project/GamePlugin-Subsystem.git: Open your project in Unity, and then open the Unity Package Manager . Click on the add (+) button, and select \"Add package from git URL\" Enter \"https://github.com/realm-ai-project/GamePlugin-Subsystem.git\" into the URL field and click \"Add\". Unity will download and install the package into your project. If these steps fail, please refer to Unity's documentation to resolve any errors. For example, there may be conflicts with the dependencies of this package with other packages in your projects, which can be manually resolved following the Unity documentation on Package Conflicts . When the package is successfully installed and there are no code compilation errors, a \"Realm AI\" menu should appear in the menu bar.","title":"Installing the Unity Package"},{"location":"game_plugin_subsystem/installation_unity/#installing-the-unity-package","text":"To install the Unity Package, use the Unity Package Manager to install it from the GitHub repository https://github.com/realm-ai-project/GamePlugin-Subsystem.git: Open your project in Unity, and then open the Unity Package Manager . Click on the add (+) button, and select \"Add package from git URL\" Enter \"https://github.com/realm-ai-project/GamePlugin-Subsystem.git\" into the URL field and click \"Add\". Unity will download and install the package into your project. If these steps fail, please refer to Unity's documentation to resolve any errors. For example, there may be conflicts with the dependencies of this package with other packages in your projects, which can be manually resolved following the Unity documentation on Package Conflicts . When the package is successfully installed and there are no code compilation errors, a \"Realm AI\" menu should appear in the menu bar.","title":"Installing the Unity Package"},{"location":"game_plugin_subsystem/known_issues/","text":"Known Issues with the Unity Package The video recorder may record more videos than specified when resuming an existing training session.","title":"Known Issues"},{"location":"game_plugin_subsystem/known_issues/#known-issues-with-the-unity-package","text":"The video recorder may record more videos than specified when resuming an existing training session.","title":"Known Issues with the Unity Package"},{"location":"game_plugin_subsystem/overview/","text":"Game Plugin Subsystem Overview The game plugin subsystem is a Unity package. For more information on Unity packages, see the Unity manual or this tutorial for how to create one. This Game Plugin section of the documentation will assume that the reader is familiar with using Unity for game development. The first half of this section will guide the reader through installing and integrate the Unity package into their game project and the second half will explain key aspects of the code given in the package to help the reader understand and modify the code as needed.","title":"Overview"},{"location":"game_plugin_subsystem/overview/#game-plugin-subsystem-overview","text":"The game plugin subsystem is a Unity package. For more information on Unity packages, see the Unity manual or this tutorial for how to create one. This Game Plugin section of the documentation will assume that the reader is familiar with using Unity for game development. The first half of this section will guide the reader through installing and integrate the Unity package into their game project and the second half will explain key aspects of the code given in the package to help the reader understand and modify the code as needed.","title":"Game Plugin Subsystem Overview"},{"location":"python_gui/description/","text":"The Python GUI acts as a bridge to connect the Game Plugin Subsystem with the RL Subsystem. It provides an interface for the user to accomplish 3 tasks: Start barebones ML-Agents Training Start Hyperparameter Tuning and use the new tuned values to start ML-Agents Training Open the results dashboard Basic ML-Agents Training The first use case of the Python GUI is to start basic ML-Agents Training. Users will have the option to configure different settings to suit their training requirements. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the ML-Agents Configuration file Start and Resume ML-Agents training Please refer to the user guide for a step by step workflow. Hyperparameter Tuning and Effective ML-Agents Training The second use case of the Python GUI is to start hyperparameter tuning and then ML-Agents training. The result of the hyperparameter tuning is an optimized ML-Agents configuration file. Once the hyperparameter tuning is done, ML-Agents training is automatically ran with the optimized configuration file. There is a difference between the two options. Simply put, the first option is to just train. The basic ML-Agents training runs the configuration file supplied by the user. The second option is to find the best parameters for training (this process is known as tuning) and then train. The Hyperparameter Tuning and Effective ML-Agents Training will find the optimized values for ML-Agents training to produce the best results. Note that this option will take significantly longer. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the Hyperparameter Configuration file Start and Resume Hyperparameter Tuning and Training Please refer to the user guide for a step by step workflow. Dashboard The third use case of the Python GUI is to start the dashboard backend to start viewing the results of the training.","title":"Description"},{"location":"python_gui/description/#basic-ml-agents-training","text":"The first use case of the Python GUI is to start basic ML-Agents Training. Users will have the option to configure different settings to suit their training requirements. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the ML-Agents Configuration file Start and Resume ML-Agents training Please refer to the user guide for a step by step workflow.","title":"Basic ML-Agents Training"},{"location":"python_gui/description/#hyperparameter-tuning-and-effective-ml-agents-training","text":"The second use case of the Python GUI is to start hyperparameter tuning and then ML-Agents training. The result of the hyperparameter tuning is an optimized ML-Agents configuration file. Once the hyperparameter tuning is done, ML-Agents training is automatically ran with the optimized configuration file. There is a difference between the two options. Simply put, the first option is to just train. The basic ML-Agents training runs the configuration file supplied by the user. The second option is to find the best parameters for training (this process is known as tuning) and then train. The Hyperparameter Tuning and Effective ML-Agents Training will find the optimized values for ML-Agents training to produce the best results. Note that this option will take significantly longer. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the Hyperparameter Configuration file Start and Resume Hyperparameter Tuning and Training Please refer to the user guide for a step by step workflow.","title":"Hyperparameter Tuning and Effective ML-Agents Training"},{"location":"python_gui/description/#dashboard","text":"The third use case of the Python GUI is to start the dashboard backend to start viewing the results of the training.","title":"Dashboard"},{"location":"python_gui/user_guide/","text":"Generic Information By hovering over the green question mark for each setting, the recommended typical values are displayed. These values come from ML-Agents Toolkit . ML-Agents Training Workflow Step 1: Adjust the settings accordingly. Visit ML-Agents Toolkit for futher information. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save ML-Agents Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start ML-Agents training, press the Start Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and the training will start! Step 6: There is also an option to continue from a previous run. Select either Initialize from previous run OR Continue previous run . Step 7: The Initialize from previous run OR Continue previous run option require a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the training will resume. Hyperparameter Tuning and Effective ML-Agents Training Workflow Step 1: Adjust the settings accordingly. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save Hyperparameter Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start Hyperparameter Tuning and ML-Agents Training, press the Start Hyperparameter Tuning and Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and tune & training will start! Step 6: There is also an option to continue from a previous run. Select Continue previous run . Step 7: The Continue previous run option requires a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the tune & training will resume. Dashboard worflow Step 1: Press the Open Dashboard button to start the local web app.","title":"User Guide"},{"location":"python_gui/user_guide/#generic-information","text":"By hovering over the green question mark for each setting, the recommended typical values are displayed. These values come from ML-Agents Toolkit .","title":"Generic Information"},{"location":"python_gui/user_guide/#ml-agents-training-workflow","text":"Step 1: Adjust the settings accordingly. Visit ML-Agents Toolkit for futher information. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save ML-Agents Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start ML-Agents training, press the Start Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and the training will start! Step 6: There is also an option to continue from a previous run. Select either Initialize from previous run OR Continue previous run . Step 7: The Initialize from previous run OR Continue previous run option require a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the training will resume.","title":"ML-Agents Training Workflow"},{"location":"python_gui/user_guide/#hyperparameter-tuning-and-effective-ml-agents-training-workflow","text":"Step 1: Adjust the settings accordingly. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save Hyperparameter Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start Hyperparameter Tuning and ML-Agents Training, press the Start Hyperparameter Tuning and Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and tune & training will start! Step 6: There is also an option to continue from a previous run. Select Continue previous run . Step 7: The Continue previous run option requires a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the tune & training will resume.","title":"Hyperparameter Tuning and Effective ML-Agents Training Workflow"},{"location":"python_gui/user_guide/#dashboard-worflow","text":"Step 1: Press the Open Dashboard button to start the local web app.","title":"Dashboard worflow"},{"location":"reporting_subsystem/description/","text":"The main goal of the reporting subsystem is to be a graphical interface to view collected data on their game. Overview of the Reporting Subsystem Key Features APIs GET /count_dat_files url parameters: {} This endpoint gets the total number of .dat files. This will inform the frontend of how many different files we have to display to the user. This will also inform the frontend of the range of the dat file id - which is used to refer to specific files. For example, a dat file id of 0 means generate heatmap for the first dat file. /by_reward/\\ /\\ /\\ url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their reward for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id. /by_episode_length/\\ /\\ /\\ url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their length for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id. /naive/ url parameters: { \"dat_id\": \"int\" } This endpoint returns the heatmap for a given dat file id. Parameters is a dat file id. /last_position/ url parameters: { \"dat_id\": \"int\" } This endpoint returns a heatmap showing the last position of every episode in a dat file. Parameters is a dat file id.","title":"Description"},{"location":"reporting_subsystem/description/#overview-of-the-reporting-subsystem","text":"","title":"Overview of the Reporting Subsystem"},{"location":"reporting_subsystem/description/#key-features","text":"","title":"Key Features"},{"location":"reporting_subsystem/description/#apis","text":"","title":"APIs"},{"location":"reporting_subsystem/description/#get","text":"","title":"GET"},{"location":"reporting_subsystem/description/#count_dat_files","text":"url parameters: {} This endpoint gets the total number of .dat files. This will inform the frontend of how many different files we have to display to the user. This will also inform the frontend of the range of the dat file id - which is used to refer to specific files. For example, a dat file id of 0 means generate heatmap for the first dat file.","title":"/count_dat_files"},{"location":"reporting_subsystem/description/#by_reward","text":"url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their reward for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id.","title":"/by_reward/\\/\\/\\"},{"location":"reporting_subsystem/description/#by_episode_length","text":"url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their length for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id.","title":"/by_episode_length/\\/\\/\\"},{"location":"reporting_subsystem/description/#naive","text":"url parameters: { \"dat_id\": \"int\" } This endpoint returns the heatmap for a given dat file id. Parameters is a dat file id.","title":"/naive/"},{"location":"reporting_subsystem/description/#last_position","text":"url parameters: { \"dat_id\": \"int\" } This endpoint returns a heatmap showing the last position of every episode in a dat file. Parameters is a dat file id.","title":"/last_position/"},{"location":"rl_subsystem/description/","text":"In this section, we outline the purpose of the RL Subsystem, and provide an introduction to the different components of the system. The main goal of the RL subsystem is to allow users to use reinforcement learning (RL) algorithms to train agents to learn to play the game. While the RL Subsystem can be used completely independently of other subsystems, the recommended use of this subsystem is to be used in conjunction with the rest of the subsystems. Overview of the RL Subsystem The RL Subsystem is made of two main components: the Training Manager (realm-tune) and a Weights and Biases wrapper (wandb-mlagents-learn) for Unity's ML-Agents Python package . Training Manager (realm-tune) Since one of the main goals of this project is to ensure that game developers/designers can use the tool without requiring extensive knowledge in RL, the training manager is meant to further simplify the process of using the underlying ML-Agents Python package. This includes the main feature of hyperparameter tuning , which allows users to use the RL algorithms without the need to manually select the hyperparameters. Weights and Biases Wrapper (wandb-mlagents-learn) Wandb-mlagents-learn is a mini wrapper library for the ML-Agents Python package , and it provides the functionality for users to track ML-Agents experiments using Weights and Biases. Please refer to the official getting started guide for more information Weights and Biases, and why it might be a good idea to use it. Even though wandb-mlagents-learn is a standalone wrapper, it has nicely integrated with The training manager ( realm-tune ), described above. ML-Agents Python Package Built by Unity , the ML-Agents Python package is part of the ML-Agents Toolkit , and contains implementations of several commonly used RL algorithms. For more information, please refer to the official documentation of ML-Agents .","title":"Description"},{"location":"rl_subsystem/description/#overview-of-the-rl-subsystem","text":"The RL Subsystem is made of two main components: the Training Manager (realm-tune) and a Weights and Biases wrapper (wandb-mlagents-learn) for Unity's ML-Agents Python package .","title":"Overview of the RL Subsystem"},{"location":"rl_subsystem/description/#training-manager-realm-tune","text":"Since one of the main goals of this project is to ensure that game developers/designers can use the tool without requiring extensive knowledge in RL, the training manager is meant to further simplify the process of using the underlying ML-Agents Python package. This includes the main feature of hyperparameter tuning , which allows users to use the RL algorithms without the need to manually select the hyperparameters.","title":"Training Manager (realm-tune)"},{"location":"rl_subsystem/description/#weights-and-biases-wrapper-wandb-mlagents-learn","text":"Wandb-mlagents-learn is a mini wrapper library for the ML-Agents Python package , and it provides the functionality for users to track ML-Agents experiments using Weights and Biases. Please refer to the official getting started guide for more information Weights and Biases, and why it might be a good idea to use it. Even though wandb-mlagents-learn is a standalone wrapper, it has nicely integrated with The training manager ( realm-tune ), described above.","title":"Weights and Biases Wrapper (wandb-mlagents-learn)"},{"location":"rl_subsystem/description/#ml-agents-python-package","text":"Built by Unity , the ML-Agents Python package is part of the ML-Agents Toolkit , and contains implementations of several commonly used RL algorithms. For more information, please refer to the official documentation of ML-Agents .","title":"ML-Agents Python Package"},{"location":"rl_subsystem/getting_started/","text":"Note: This quick start guide is only applicable for using this RL Subsystem as a standalone. If using in conjunction with the REALM_AI's game plugin, the below steps are already automated for you. Building the environment The first step is to build a ML-Agents compatible Unity environment, and build it into an executable . Here is a tutorial of building a Unity ML-Agents compatible environment from the official ML-Agents documentation. Remember where the path of the executable is, because we will need to use it in the later steps! Installing REALM_AI's RL-Subsystem In terminal, enter the following commands to pull and setup the python package (e.g., install the necessary dependencies). git clone git@github.com:realm-ai-project/RL-Subsystem.git cd RL-Subsystem pip install -e . Training While what makes realm-tune powerful is its configurability, to get started it is as simple as performing the following command in the terminal: realm-tune --env-path <env_path> where <env_path> is replaced with the path to the unity executable built above. If you see a lot of output to the console, it means the program is working as expected! You may see a tiny game window show up from time to time, and that is to be expected! If you wait for a while, the program will eventually be completed, and all training outputs will be stored at runs/RealmTune_xx-xx-xxxx_xx-xx-xx , where the x represents a datetime stamp so that your training outputs are always easily recognizable. Congrats, you have tuned your first agent! Learn More There are much more that realm-tune can do that will be covered in the subsequent pages, such as configuring hyperparameters to tune over picking the hyperparameter tuning algorithms integrate with Weights and Biases with wandb-mlagents-learn automatically initiate the full training run after the hyperparameter tuning stage, using the best set of hyperparameters many more","title":"Quick Start Guide"},{"location":"rl_subsystem/getting_started/#building-the-environment","text":"The first step is to build a ML-Agents compatible Unity environment, and build it into an executable . Here is a tutorial of building a Unity ML-Agents compatible environment from the official ML-Agents documentation. Remember where the path of the executable is, because we will need to use it in the later steps!","title":"Building the environment"},{"location":"rl_subsystem/getting_started/#installing-realm_ais-rl-subsystem","text":"In terminal, enter the following commands to pull and setup the python package (e.g., install the necessary dependencies). git clone git@github.com:realm-ai-project/RL-Subsystem.git cd RL-Subsystem pip install -e .","title":"Installing REALM_AI's RL-Subsystem"},{"location":"rl_subsystem/getting_started/#training","text":"While what makes realm-tune powerful is its configurability, to get started it is as simple as performing the following command in the terminal: realm-tune --env-path <env_path> where <env_path> is replaced with the path to the unity executable built above. If you see a lot of output to the console, it means the program is working as expected! You may see a tiny game window show up from time to time, and that is to be expected! If you wait for a while, the program will eventually be completed, and all training outputs will be stored at runs/RealmTune_xx-xx-xxxx_xx-xx-xx , where the x represents a datetime stamp so that your training outputs are always easily recognizable. Congrats, you have tuned your first agent!","title":"Training"},{"location":"rl_subsystem/getting_started/#learn-more","text":"There are much more that realm-tune can do that will be covered in the subsequent pages, such as configuring hyperparameters to tune over picking the hyperparameter tuning algorithms integrate with Weights and Biases with wandb-mlagents-learn automatically initiate the full training run after the hyperparameter tuning stage, using the best set of hyperparameters many more","title":"Learn More"},{"location":"rl_subsystem/hyperparameter_tuning/","text":"What Are Hyperparameters? In machine learning, hyperparameters are parameters whose values are used to control the learning process. In other words, hyperparameters directly affect the result of the learned parameters. As a result, hyperparameters are not learned during the training process. Instead, hyperparameters are usually set before training, and the set of optimal hyperparameters are usually chosen empirically through a process of trial and error. This process is called hyperparameter tuning/optimization . Examples of Hyperparameters for Reinforcement Learning Some common hyperparameters include batch size, learning rate, exploration-related hyperparameters (e.g., epsilon), etc. For a more comprehensive list of hyperparameters available for various algorithms provided by Unity's ML-Agents package, please refer to here . Why Automate the Hyperparameter Tuning Process? While it is possible to manually tune hyperparameters through trial and error, this process is mundane and very expensive given the usually long training times. This process is worsen by the need for domain knowledge in order to know which hyperparameter to tune, and the scale to tune them by. Common Hyperparameter Optimization Algorithms There are a few dominant algorithms that are commonly used to automate the process of tuning the hyperparameters. This is namely Grid Search, Random/Uninformed Search, and Bayesian methods such as the Tree-structured Parzen Estimator (TPE) algorithm. Realm-tune currently supports all of them. To get started on how to automatically tune hyperparameters with REALM_AI, head to the user guide .","title":"Hyperparameter Tuning"},{"location":"rl_subsystem/hyperparameter_tuning/#what-are-hyperparameters","text":"In machine learning, hyperparameters are parameters whose values are used to control the learning process. In other words, hyperparameters directly affect the result of the learned parameters. As a result, hyperparameters are not learned during the training process. Instead, hyperparameters are usually set before training, and the set of optimal hyperparameters are usually chosen empirically through a process of trial and error. This process is called hyperparameter tuning/optimization .","title":"What Are Hyperparameters?"},{"location":"rl_subsystem/hyperparameter_tuning/#examples-of-hyperparameters-for-reinforcement-learning","text":"Some common hyperparameters include batch size, learning rate, exploration-related hyperparameters (e.g., epsilon), etc. For a more comprehensive list of hyperparameters available for various algorithms provided by Unity's ML-Agents package, please refer to here .","title":"Examples of Hyperparameters for Reinforcement Learning"},{"location":"rl_subsystem/hyperparameter_tuning/#why-automate-the-hyperparameter-tuning-process","text":"While it is possible to manually tune hyperparameters through trial and error, this process is mundane and very expensive given the usually long training times. This process is worsen by the need for domain knowledge in order to know which hyperparameter to tune, and the scale to tune them by.","title":"Why Automate the Hyperparameter Tuning Process?"},{"location":"rl_subsystem/hyperparameter_tuning/#common-hyperparameter-optimization-algorithms","text":"There are a few dominant algorithms that are commonly used to automate the process of tuning the hyperparameters. This is namely Grid Search, Random/Uninformed Search, and Bayesian methods such as the Tree-structured Parzen Estimator (TPE) algorithm. Realm-tune currently supports all of them. To get started on how to automatically tune hyperparameters with REALM_AI, head to the user guide .","title":"Common Hyperparameter Optimization Algorithms"},{"location":"rl_subsystem/user_guide/configuration/","text":"In this section, we will be going through the two main ways to interact with the realm-tune tool. Configuration file Below is an example config file that contains all possible configurations (don't worry, its not a lot!). realm_ai: algorithm: bayes # or random total_trials: 3 # total number of trials (inclusive of warmup_trials) To start, create a .yaml file with any name you would like, copy the following code block and paste it in the newly created .yaml file. Cli arguments The purpose of having cli arguments in conjunction of the configuration file is mainly for convenience, especially when realm-tune is interacted with programatically from other tools. For instance, it can be a hassle to parse the config file, save the config file, and then point realm-tune to it. One important thing to note is that arguments passed through the cli args always takes precedence over those through the config files. In other words, arguments passed through the cli always overrides those in the config file. This allows a user to say, have a fixed config file that they are comfortable with, and override them through the cli everytime they have a different game etc. To get the list of available cli arguments, do realm-tune --help , the output of which should look something like: usage: realm-tune [-h] [--config-path CONFIG_PATH] [--output-path OUTPUT_PATH] [--behavior-name BEHAVIOR_NAME] [--algorithm {bayes,random,grid}] [--total-trials TOTAL_TRIALS] [--warmup-trials WARMUP_TRIALS] [--eval-window-size EVAL_WINDOW_SIZE] [--env-path ENV_PATH] [--use-wandb] [--wandb-project WANDB_PROJECT] [--wandb-entity WANDB_ENTITY] [--wandb-offline] [--wandb-group WANDB_GROUP] [--wandb-jobtype WANDB_JOBTYPE] [--full-run] [--full-run-max-steps FULL_RUN_MAX_STEPS] Realm_AI hyperparameter optimization tool optional arguments: -h, --help show this help message and exit --config-path CONFIG_PATH --output-path OUTPUT_PATH Specify path where data is stored --behavior-name BEHAVIOR_NAME Name of behaviour. This can be found under the agent's \"Behavior Parameters\" component in the inspector of Unity --algorithm {bayes,random,grid} Algorithm for hyperparameter tuning --total-trials TOTAL_TRIALS Number of trials --warmup-trials WARMUP_TRIALS Number of warmup trials (only works for bayes algorithm) --eval-window-size EVAL_WINDOW_SIZE Training run is evaluated by taking the average eps rew of past x episodes --env-path ENV_PATH Path to environment. If specified, overrides env_path in the config file Weights and Biases Configuration: --use-wandb --wandb-project WANDB_PROJECT --wandb-entity WANDB_ENTITY --wandb-offline --wandb-group WANDB_GROUP --wandb-jobtype WANDB_JOBTYPE Full run configuration: --full-run --full-run-max-steps FULL_RUN_MAX_STEPS All arguments should look pretty familiar as they are mostly identical to those from the config file. However, the only two noteworthy arguments to point out are: --use-wandb exists because it allows the user to use Weights and Biases without requiring passing in any other information, such as the entity name. --full-run exists for the exact same reason. It allows the user to do an automated full training run after hyperparameter tuning, without the need to configure the number of steps (the default value will be used instead).","title":"Configuration"},{"location":"rl_subsystem/user_guide/configuration/#configuration-file","text":"Below is an example config file that contains all possible configurations (don't worry, its not a lot!). realm_ai: algorithm: bayes # or random total_trials: 3 # total number of trials (inclusive of warmup_trials) To start, create a .yaml file with any name you would like, copy the following code block and paste it in the newly created .yaml file.","title":"Configuration file"},{"location":"rl_subsystem/user_guide/configuration/#cli-arguments","text":"The purpose of having cli arguments in conjunction of the configuration file is mainly for convenience, especially when realm-tune is interacted with programatically from other tools. For instance, it can be a hassle to parse the config file, save the config file, and then point realm-tune to it. One important thing to note is that arguments passed through the cli args always takes precedence over those through the config files. In other words, arguments passed through the cli always overrides those in the config file. This allows a user to say, have a fixed config file that they are comfortable with, and override them through the cli everytime they have a different game etc. To get the list of available cli arguments, do realm-tune --help , the output of which should look something like: usage: realm-tune [-h] [--config-path CONFIG_PATH] [--output-path OUTPUT_PATH] [--behavior-name BEHAVIOR_NAME] [--algorithm {bayes,random,grid}] [--total-trials TOTAL_TRIALS] [--warmup-trials WARMUP_TRIALS] [--eval-window-size EVAL_WINDOW_SIZE] [--env-path ENV_PATH] [--use-wandb] [--wandb-project WANDB_PROJECT] [--wandb-entity WANDB_ENTITY] [--wandb-offline] [--wandb-group WANDB_GROUP] [--wandb-jobtype WANDB_JOBTYPE] [--full-run] [--full-run-max-steps FULL_RUN_MAX_STEPS] Realm_AI hyperparameter optimization tool optional arguments: -h, --help show this help message and exit --config-path CONFIG_PATH --output-path OUTPUT_PATH Specify path where data is stored --behavior-name BEHAVIOR_NAME Name of behaviour. This can be found under the agent's \"Behavior Parameters\" component in the inspector of Unity --algorithm {bayes,random,grid} Algorithm for hyperparameter tuning --total-trials TOTAL_TRIALS Number of trials --warmup-trials WARMUP_TRIALS Number of warmup trials (only works for bayes algorithm) --eval-window-size EVAL_WINDOW_SIZE Training run is evaluated by taking the average eps rew of past x episodes --env-path ENV_PATH Path to environment. If specified, overrides env_path in the config file Weights and Biases Configuration: --use-wandb --wandb-project WANDB_PROJECT --wandb-entity WANDB_ENTITY --wandb-offline --wandb-group WANDB_GROUP --wandb-jobtype WANDB_JOBTYPE Full run configuration: --full-run --full-run-max-steps FULL_RUN_MAX_STEPS All arguments should look pretty familiar as they are mostly identical to those from the config file. However, the only two noteworthy arguments to point out are: --use-wandb exists because it allows the user to use Weights and Biases without requiring passing in any other information, such as the entity name. --full-run exists for the exact same reason. It allows the user to do an automated full training run after hyperparameter tuning, without the need to configure the number of steps (the default value will be used instead).","title":"Cli arguments"},{"location":"rl_subsystem/user_guide/limitations/","text":"caveats, bugs e.g. single player only, interrupt does not work properly on windows etc.","title":"Limitations"},{"location":"rl_subsystem/user_guide/realm_tune/","text":"REALM_AI's RL hyperparameter optimization tool focuses on configurability and simplicity to use. There are 3 main steps required to use the tool: build the environment , edit the configuration file , and finally run the training procedure . Building the Environment The first step is to build the Unity ML-Agents compatible environment into an executable. ML-Agents' documentation contains a comprehensive guide that details a step-by-step guide to export the environment as an executable. Editing the Configuration File In order to make the tool as simple to use as possible, all settings reside within a single .yaml file. There are 2 main components in the configuration file -- configurations for the REALM_AI RL_Subsystem's hyperparameter optimizer, and configurations for ML-Agents. REALM_AI Configuration There are two main ways to interact with ML-Agents Configuration Note that we aren't doing any checking for the mlagents config portion Also note that log_unif and unif exclude upper bound Training configuration Hyperparameters Why the unconventional syntax for the hyperparameters? (To keep the yaml file concise, reduce verbosity that comes from yaml's syntax) Since Syntax Sample Configuration File Starting the Training Procedure Caveats Single behavior only The current setup assumes single-player game that contains a single behavior. Games with multiple behaviours would not work with the current script. However, due to the simplistic nature of the script, adding support for multi-agent environments is not difficult.","title":"Realm tune"},{"location":"rl_subsystem/user_guide/realm_tune/#building-the-environment","text":"The first step is to build the Unity ML-Agents compatible environment into an executable. ML-Agents' documentation contains a comprehensive guide that details a step-by-step guide to export the environment as an executable.","title":"Building the Environment"},{"location":"rl_subsystem/user_guide/realm_tune/#editing-the-configuration-file","text":"In order to make the tool as simple to use as possible, all settings reside within a single .yaml file. There are 2 main components in the configuration file -- configurations for the REALM_AI RL_Subsystem's hyperparameter optimizer, and configurations for ML-Agents.","title":"Editing the Configuration File"},{"location":"rl_subsystem/user_guide/realm_tune/#realm_ai-configuration","text":"There are two main ways to interact with","title":"REALM_AI Configuration"},{"location":"rl_subsystem/user_guide/realm_tune/#ml-agents-configuration","text":"Note that we aren't doing any checking for the mlagents config portion Also note that log_unif and unif exclude upper bound","title":"ML-Agents Configuration"},{"location":"rl_subsystem/user_guide/realm_tune/#training-configuration","text":"","title":"Training configuration"},{"location":"rl_subsystem/user_guide/realm_tune/#hyperparameters","text":"","title":"Hyperparameters"},{"location":"rl_subsystem/user_guide/realm_tune/#why-the-unconventional-syntax-for-the-hyperparameters","text":"(To keep the yaml file concise, reduce verbosity that comes from yaml's syntax) Since","title":"Why the unconventional syntax for the hyperparameters?"},{"location":"rl_subsystem/user_guide/realm_tune/#syntax","text":"","title":"Syntax"},{"location":"rl_subsystem/user_guide/realm_tune/#sample-configuration-file","text":"","title":"Sample Configuration File"},{"location":"rl_subsystem/user_guide/realm_tune/#starting-the-training-procedure","text":"","title":"Starting the Training Procedure"},{"location":"rl_subsystem/user_guide/realm_tune/#caveats","text":"","title":"Caveats"},{"location":"rl_subsystem/user_guide/realm_tune/#single-behavior-only","text":"The current setup assumes single-player game that contains a single behavior. Games with multiple behaviours would not work with the current script. However, due to the simplistic nature of the script, adding support for multi-agent environments is not difficult.","title":"Single behavior only"},{"location":"rl_subsystem/user_guide/resuming/","text":"Talk about pausing and resuming behaviour","title":"Resuming"}]}