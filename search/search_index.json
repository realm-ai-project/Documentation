{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to REALM_AI's documentation!","title":"Introduction"},{"location":"#welcome-to-realm_ais-documentation","text":"","title":"Welcome to REALM_AI's documentation!"},{"location":"installation/","text":"Installation Guide","title":"Installation"},{"location":"installation/#installation-guide","text":"","title":"Installation Guide"},{"location":"analysis_subsystem/description/","text":"The main goal of the Analysis subsystem is to automatically create different heatmaps based on the data generated upstream of the pipeline.","title":"Description"},{"location":"analysis_subsystem/heatmaps/","text":"What Are Heatmaps? Hello world Heatmap 1 - Generic Heatmap I wanna eat Heatmap 2 - I wanna sleep Heatmap 3 - I wanna cry Heatmap 4 - I wanna die","title":"Heatmaps"},{"location":"analysis_subsystem/heatmaps/#what-are-heatmaps","text":"Hello world","title":"What Are Heatmaps?"},{"location":"analysis_subsystem/heatmaps/#heatmap-1-generic-heatmap","text":"I wanna eat","title":"Heatmap 1 - Generic Heatmap"},{"location":"analysis_subsystem/heatmaps/#heatmap-2-","text":"I wanna sleep","title":"Heatmap 2 -"},{"location":"analysis_subsystem/heatmaps/#heatmap-3-","text":"I wanna cry","title":"Heatmap 3 -"},{"location":"analysis_subsystem/heatmaps/#heatmap-4-","text":"I wanna die","title":"Heatmap 4 -"},{"location":"feature_request/todo/","text":"Game Subsystem If memory becomes a problem due to size of .dat file, adjust logging frequency Reporting Subsystem Embed TensorBoard as an IFrame ( example ) within the reporting subsystem so that users do not need to go to a different website Range slider: the idea is to allow users to select the timeframe window of which they would like the heatmap to be plotted over Example: Screenshot from Plotly Related to the range slider, but a more conventional slider that allows users to view captured videos/screenshots on a timeline Example: Screenshot from Weights & Biases For the data storage file format, consider creating an EBML based format, which combines the space-efficiency of pure binary formats and the flexbility/forwards+backwards compatibility of json. The main drawbacks is that it is not easily humanly readable and may be more complex to read and write. Notable existing formats based on EBML are WEBM and MKV.","title":"Feature Request"},{"location":"feature_request/todo/#game-subsystem","text":"If memory becomes a problem due to size of .dat file, adjust logging frequency","title":"Game Subsystem"},{"location":"feature_request/todo/#reporting-subsystem","text":"Embed TensorBoard as an IFrame ( example ) within the reporting subsystem so that users do not need to go to a different website Range slider: the idea is to allow users to select the timeframe window of which they would like the heatmap to be plotted over Example: Screenshot from Plotly Related to the range slider, but a more conventional slider that allows users to view captured videos/screenshots on a timeline Example: Screenshot from Weights & Biases For the data storage file format, consider creating an EBML based format, which combines the space-efficiency of pure binary formats and the flexbility/forwards+backwards compatibility of json. The main drawbacks is that it is not easily humanly readable and may be more complex to read and write. Notable existing formats based on EBML are WEBM and MKV.","title":"Reporting Subsystem"},{"location":"game_plugin_subsystem/editor_scripts/","text":"Editor Scripts Breakdown coming soon...","title":"Editor Scripts Breakdown"},{"location":"game_plugin_subsystem/editor_scripts/#editor-scripts-breakdown","text":"coming soon...","title":"Editor  Scripts Breakdown"},{"location":"game_plugin_subsystem/installation/","text":"Installation The installation process for the users is detailed in the image below, which should also provide an overview of how the package is used. For step 4, the configuration required of the user is detailed in the Prefab Breakdown page. The Unity package is designed to be installed simply from its git URL using Unity's built-in package management system. The package is also designed to be able to be installed into an existing game project. Should there be conflicts with other installed packages, this documentation currently does not provide any details on resolving potential conflicts with other packages. Installation for Development For development, instead of using the \"Add package from git URL...\" option, clone the repo separately and use the \"Add package from disk...\" option. In the dialog popup, select the package.json file from the cloned repo. This will install directly from the cloned repo. You can then directly modify the package from inside Unity and the changes will be made directly to the files in the cloned repo.","title":"Installation"},{"location":"game_plugin_subsystem/installation/#installation","text":"The installation process for the users is detailed in the image below, which should also provide an overview of how the package is used. For step 4, the configuration required of the user is detailed in the Prefab Breakdown page. The Unity package is designed to be installed simply from its git URL using Unity's built-in package management system. The package is also designed to be able to be installed into an existing game project. Should there be conflicts with other installed packages, this documentation currently does not provide any details on resolving potential conflicts with other packages.","title":"Installation"},{"location":"game_plugin_subsystem/installation/#installation-for-development","text":"For development, instead of using the \"Add package from git URL...\" option, clone the repo separately and use the \"Add package from disk...\" option. In the dialog popup, select the package.json file from the cloned repo. This will install directly from the cloned repo. You can then directly modify the package from inside Unity and the changes will be made directly to the files in the cloned repo.","title":"Installation for Development"},{"location":"game_plugin_subsystem/installation_configuration/","text":"Configuring the Realm AI Module All of the configuration needed for RealmAI features are stored within the instance of Realm AI Module (which should be a child of the player prefab). There are a number of things to configure in the Realm AI Module before the RealmAI features will work. To help with this, all of the essential configuration and inspector fields that need to be filled will be shown (mirrored) in the Configuration Window itself, so you don't have to search through the Realm AI Module to find them. These inspectors fields need to be filled before the RealmAI features will work:","title":"Configuring the Realm AI Module"},{"location":"game_plugin_subsystem/installation_configuration/#configuring-the-realm-ai-module","text":"All of the configuration needed for RealmAI features are stored within the instance of Realm AI Module (which should be a child of the player prefab). There are a number of things to configure in the Realm AI Module before the RealmAI features will work. To help with this, all of the essential configuration and inspector fields that need to be filled will be shown (mirrored) in the Configuration Window itself, so you don't have to search through the Realm AI Module to find them. These inspectors fields need to be filled before the RealmAI features will work:","title":"Configuring the Realm AI Module"},{"location":"game_plugin_subsystem/installation_player/","text":"Project Requirements, Setting up the Player Prefab To setup RealmAI, it requires there to be a prefab representing the player in the game project. This prefab should: - represent the player, i.e. its position represents the player's position in the game world - be present in the game Scene or be spawned in as soon as the application is run - have only one instance in the game environment throughout the game - ensure that its one instance is never deactivated, disabled, or destroyed throughout the game If your project does not have a player prefab that satisfy these requirements, please create one, or adjust your code so that the requirements are satisfied. If these requirements cannot be satisfied, your project may not be suitable for RealmAI \ud83d\ude22. Once you have a player prefab that satisfies the requirements above, let RealmAI know about this prefab by assigning the prefab to the \"Player Prefab\" field in the Configuration Window in Unity (found under Realm AI/Open Configuration Window), which should look like this: To continue, open click \"Open player prefab for edit\" to open the player prefab in prefab edit mode. Once in prefab edit mode, click on \"Add Realm AI Module\" to add an instance of the \"Realm AI Module\" prefab (included with the RealmAI package) as a child of the player prefab. This Realm AI Module contains a series of scripts for RealmAI features, which also store all of the configuration needed for these features. Note: like with the player, there should only be one active instance of the Realm AI Module throughout the game. If you have prefab variants that derive from the player prefab, please disable the Realm AI Module in those prefabs variants.","title":"Project Requirements, Setting up the Player Prefab"},{"location":"game_plugin_subsystem/installation_player/#project-requirements-setting-up-the-player-prefab","text":"To setup RealmAI, it requires there to be a prefab representing the player in the game project. This prefab should: - represent the player, i.e. its position represents the player's position in the game world - be present in the game Scene or be spawned in as soon as the application is run - have only one instance in the game environment throughout the game - ensure that its one instance is never deactivated, disabled, or destroyed throughout the game If your project does not have a player prefab that satisfy these requirements, please create one, or adjust your code so that the requirements are satisfied. If these requirements cannot be satisfied, your project may not be suitable for RealmAI \ud83d\ude22. Once you have a player prefab that satisfies the requirements above, let RealmAI know about this prefab by assigning the prefab to the \"Player Prefab\" field in the Configuration Window in Unity (found under Realm AI/Open Configuration Window), which should look like this: To continue, open click \"Open player prefab for edit\" to open the player prefab in prefab edit mode. Once in prefab edit mode, click on \"Add Realm AI Module\" to add an instance of the \"Realm AI Module\" prefab (included with the RealmAI package) as a child of the player prefab. This Realm AI Module contains a series of scripts for RealmAI features, which also store all of the configuration needed for these features. Note: like with the player, there should only be one active instance of the Realm AI Module throughout the game. If you have prefab variants that derive from the player prefab, please disable the Realm AI Module in those prefabs variants.","title":"Project Requirements, Setting up the Player Prefab"},{"location":"game_plugin_subsystem/installation_python/","text":"Setting up the Python Environment The Unity Package requires the system to have Python 3 installed (this package is tested with version 3.7, but other versions may work), and to have the Python packages realm-tune , realm-gui , and realm-report installed. You can verify that these are installed by opening a command line and entering the commands realm-tune , realm-gui , or realm-report . They should all run and do something if installed correctly. Setting up the Python Environment It is often recommended to use virtual or isolated environments when using Python. This documentation will not explain why or how to do this, but if you have installed the Python dependencies into a specific environment, Realm AI needs to be able to activate the environment to use these dependencies. To set this up, open the Configuration Window in Unity (found under Realm AI/Open Configuration Window) and near the bottom, enter the command(s) to activate the environment in the \"Environment Setup Command\" field. An example of the setup for a Conda environment named \"ml\", on Windows, is shown below: If you use Python without any environments, just leave this field blank.","title":"Setting up the Python Environment"},{"location":"game_plugin_subsystem/installation_python/#setting-up-the-python-environment","text":"The Unity Package requires the system to have Python 3 installed (this package is tested with version 3.7, but other versions may work), and to have the Python packages realm-tune , realm-gui , and realm-report installed. You can verify that these are installed by opening a command line and entering the commands realm-tune , realm-gui , or realm-report . They should all run and do something if installed correctly.","title":"Setting up the Python Environment"},{"location":"game_plugin_subsystem/installation_python/#setting-up-the-python-environment_1","text":"It is often recommended to use virtual or isolated environments when using Python. This documentation will not explain why or how to do this, but if you have installed the Python dependencies into a specific environment, Realm AI needs to be able to activate the environment to use these dependencies. To set this up, open the Configuration Window in Unity (found under Realm AI/Open Configuration Window) and near the bottom, enter the command(s) to activate the environment in the \"Environment Setup Command\" field. An example of the setup for a Conda environment named \"ml\", on Windows, is shown below: If you use Python without any environments, just leave this field blank.","title":"Setting up the Python Environment"},{"location":"game_plugin_subsystem/installation_unity/","text":"Installing the Unity Package To install the Unity Package, use the Unity Package Manager to install it from the GitHub repository https://github.com/realm-ai-project/GamePlugin-Subsystem.git: Open your project in Unity, and then open the Unity Package Manager . Click on the add (+) button, and select \"Add package from git URL\" Enter \"https://github.com/realm-ai-project/GamePlugin-Subsystem.git\" into the URL field and click \"Add\". Unity will download and install the package into your project. If these steps fail, please refer to Unity's documentation to resolve any errors. For example, there may be conflicts with the dependencies of this package with other packages in your projects, which can be manually resolved following the Unity documentation on Package Conflicts . When the package is successfully installed and there are no code compilation errors, a \"Realm AI\" menu should appear in the menu bar.","title":"Installing the Unity Package"},{"location":"game_plugin_subsystem/installation_unity/#installing-the-unity-package","text":"To install the Unity Package, use the Unity Package Manager to install it from the GitHub repository https://github.com/realm-ai-project/GamePlugin-Subsystem.git: Open your project in Unity, and then open the Unity Package Manager . Click on the add (+) button, and select \"Add package from git URL\" Enter \"https://github.com/realm-ai-project/GamePlugin-Subsystem.git\" into the URL field and click \"Add\". Unity will download and install the package into your project. If these steps fail, please refer to Unity's documentation to resolve any errors. For example, there may be conflicts with the dependencies of this package with other packages in your projects, which can be manually resolved following the Unity documentation on Package Conflicts . When the package is successfully installed and there are no code compilation errors, a \"Realm AI\" menu should appear in the menu bar.","title":"Installing the Unity Package"},{"location":"game_plugin_subsystem/overview/","text":"Game Plugin Subsystem Overview The game plugin subsystem is a Unity package. For more information on Unity packages, see the Unity manual or this tutorial for how to create one. This Game Plugin section of the documentation will assume that the reader is familiar with using Unity for game development. The first half of this section will guide the reader through installing and integrate the Unity package into their game project and the second half will explain key aspects of the code given in the package to help the reader understand and modify the code as needed.","title":"Overview"},{"location":"game_plugin_subsystem/overview/#game-plugin-subsystem-overview","text":"The game plugin subsystem is a Unity package. For more information on Unity packages, see the Unity manual or this tutorial for how to create one. This Game Plugin section of the documentation will assume that the reader is familiar with using Unity for game development. The first half of this section will guide the reader through installing and integrate the Unity package into their game project and the second half will explain key aspects of the code given in the package to help the reader understand and modify the code as needed.","title":"Game Plugin Subsystem Overview"},{"location":"game_plugin_subsystem/prefab_breakdown/","text":"Realm AI Module Prefab Breakdown The \"Realm AI Module\" is a Unity prefab included with our package. The user should drag this into their project as a child of their player object. The following diagram is a breakdown of the different components on the prefab, as well as the setup that the user has to perform.","title":"Prefab Breakdown"},{"location":"game_plugin_subsystem/prefab_breakdown/#realm-ai-module-prefab-breakdown","text":"The \"Realm AI Module\" is a Unity prefab included with our package. The user should drag this into their project as a child of their player object. The following diagram is a breakdown of the different components on the prefab, as well as the setup that the user has to perform.","title":"Realm AI Module Prefab Breakdown"},{"location":"game_plugin_subsystem/runtime_scripts/","text":"Runtime Scripts Breakdown The REALM_AI Unity package heavily depends on the ML-Agents Unity package for its machine learning features. The ML-Agents Unity package is mainly composed of Unity scripts that are required for specifying the behaviour of the machine learning agent and establishing a connection with the Python machine learning environment when training the agent. The REALM_AI Unity package contains scripts that will perform the required setup for ML-Agents (in addition to doing it's own thing). For more information on ML-Agents, see its official documentation . This subsection of the documentation is intended for readers with some knowledge of ML-Agents and users who want to understand and modify the scripts in the REALM_AI package. RealmAgent.cs RealmSensorComponent.cs RealmActuatorComponent.cs RealmOwl.cs RealmRecorder.cs","title":"Runtime Scripts Breakdown"},{"location":"game_plugin_subsystem/runtime_scripts/#runtime-scripts-breakdown","text":"The REALM_AI Unity package heavily depends on the ML-Agents Unity package for its machine learning features. The ML-Agents Unity package is mainly composed of Unity scripts that are required for specifying the behaviour of the machine learning agent and establishing a connection with the Python machine learning environment when training the agent. The REALM_AI Unity package contains scripts that will perform the required setup for ML-Agents (in addition to doing it's own thing). For more information on ML-Agents, see its official documentation . This subsection of the documentation is intended for readers with some knowledge of ML-Agents and users who want to understand and modify the scripts in the REALM_AI package.","title":"Runtime Scripts Breakdown"},{"location":"game_plugin_subsystem/runtime_scripts/#realmagentcs","text":"","title":"RealmAgent.cs"},{"location":"game_plugin_subsystem/runtime_scripts/#realmsensorcomponentcs","text":"","title":"RealmSensorComponent.cs"},{"location":"game_plugin_subsystem/runtime_scripts/#realmactuatorcomponentcs","text":"","title":"RealmActuatorComponent.cs"},{"location":"game_plugin_subsystem/runtime_scripts/#realmowlcs","text":"","title":"RealmOwl.cs"},{"location":"game_plugin_subsystem/runtime_scripts/#realmrecordercs","text":"","title":"RealmRecorder.cs"},{"location":"python_gui/description/","text":"The Python GUI acts as a bridge to connect the Game Plugin Subsystem with the RL Subsystem. It provides an interface for the user to accomplish 3 tasks: Start barebones ML-Agents Training Start Hyperparameter Tuning and use the new tuned values to start ML-Agents Training Open the results dashboard Basic ML-Agents Training The first use case of the Python GUI is to start basic ML-Agents Training. Users will have the option to configure different settings to suit their training requirements. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the ML-Agents Configuration file Start and Resume ML-Agents training Please refer to the user guide for a step by step workflow. Hyperparameter Tuning and Effective ML-Agents Training The second use case of the Python GUI is to start hyperparameter tuning and then ML-Agents training. The result of the hyperparameter tuning is an optimized ML-Agents configuration file. Once the hyperparameter tuning is done, ML-Agents training is automatically ran with the optimized configuration file. There is a difference between the two options. Simply put, the first option is to just train. The basic ML-Agents training runs the configuration file supplied by the user. The second option is to find the best parameters for training (this process is known as tuning) and then train. The Hyperparameter Tuning and Effective ML-Agents Training will find the optimized values for ML-Agents training to produce the best results. Note that this option will take significantly longer. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the Hyperparameter Configuration file Start and Resume Hyperparameter Tuning and Training Please refer to the user guide for a step by step workflow. Dashboard The third use case of the Python GUI is to start the dashboard backend to start viewing the results of the training.","title":"Description"},{"location":"python_gui/description/#basic-ml-agents-training","text":"The first use case of the Python GUI is to start basic ML-Agents Training. Users will have the option to configure different settings to suit their training requirements. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the ML-Agents Configuration file Start and Resume ML-Agents training Please refer to the user guide for a step by step workflow.","title":"Basic ML-Agents Training"},{"location":"python_gui/description/#hyperparameter-tuning-and-effective-ml-agents-training","text":"The second use case of the Python GUI is to start hyperparameter tuning and then ML-Agents training. The result of the hyperparameter tuning is an optimized ML-Agents configuration file. Once the hyperparameter tuning is done, ML-Agents training is automatically ran with the optimized configuration file. There is a difference between the two options. Simply put, the first option is to just train. The basic ML-Agents training runs the configuration file supplied by the user. The second option is to find the best parameters for training (this process is known as tuning) and then train. The Hyperparameter Tuning and Effective ML-Agents Training will find the optimized values for ML-Agents training to produce the best results. Note that this option will take significantly longer. There are 3 buttons at the bottom that can be clicked. They can: Restore Default Values Save the Hyperparameter Configuration file Start and Resume Hyperparameter Tuning and Training Please refer to the user guide for a step by step workflow.","title":"Hyperparameter Tuning and Effective ML-Agents Training"},{"location":"python_gui/description/#dashboard","text":"The third use case of the Python GUI is to start the dashboard backend to start viewing the results of the training.","title":"Dashboard"},{"location":"python_gui/user_guide/","text":"Generic Information By hovering over the green question mark for each setting, the recommended typical values are displayed. These values come from ML-Agents Toolkit . ML-Agents Training Workflow Step 1: Adjust the settings accordingly. Visit ML-Agents Toolkit for futher information. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save ML-Agents Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start ML-Agents training, press the Start Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and the training will start! Step 6: There is also an option to continue from a previous run. Select either Initialize from previous run OR Continue previous run . Step 7: The Initialize from previous run OR Continue previous run option require a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the training will resume. Hyperparameter Tuning and Effective ML-Agents Training Workflow Step 1: Adjust the settings accordingly. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save Hyperparameter Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start Hyperparameter Tuning and ML-Agents Training, press the Start Hyperparameter Tuning and Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and tune & training will start! Step 6: There is also an option to continue from a previous run. Select Continue previous run . Step 7: The Continue previous run option requires a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the tune & training will resume. Dashboard worflow Step 1: Press the Open Dashboard button to start the local web app.","title":"User Guide"},{"location":"python_gui/user_guide/#generic-information","text":"By hovering over the green question mark for each setting, the recommended typical values are displayed. These values come from ML-Agents Toolkit .","title":"Generic Information"},{"location":"python_gui/user_guide/#ml-agents-training-workflow","text":"Step 1: Adjust the settings accordingly. Visit ML-Agents Toolkit for futher information. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save ML-Agents Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start ML-Agents training, press the Start Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and the training will start! Step 6: There is also an option to continue from a previous run. Select either Initialize from previous run OR Continue previous run . Step 7: The Initialize from previous run OR Continue previous run option require a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the training will resume.","title":"ML-Agents Training Workflow"},{"location":"python_gui/user_guide/#hyperparameter-tuning-and-effective-ml-agents-training-workflow","text":"Step 1: Adjust the settings accordingly. Step 2: Change the configuration file name. If the name is not changed, the configuration file will be named config.yaml . Press the Save Hyperparameter Configuration button and this will create the configuration file. To reset the settings, press the Restore Defaults button. Step 3: To start Hyperparameter Tuning and ML-Agents Training, press the Start Hyperparameter Tuning and Training button. Step 4: Select a configuration file. Step 5: Click the Proceed button and tune & training will start! Step 6: There is also an option to continue from a previous run. Select Continue previous run . Step 7: The Continue previous run option requires a Previous Run Results Directory . Navigate to the file system and paste the path in. Press Proceed and the tune & training will resume.","title":"Hyperparameter Tuning and Effective ML-Agents Training Workflow"},{"location":"python_gui/user_guide/#dashboard-worflow","text":"Step 1: Press the Open Dashboard button to start the local web app.","title":"Dashboard worflow"},{"location":"reporting_subsystem/description/","text":"The main goal of the reporting subsystem is to be a graphical interface to view collected data on their game. Overview of the Reporting Subsystem Key Features APIs GET /count_dat_files url parameters: {} This endpoint gets the total number of .dat files. This will inform the frontend of how many different files we have to display to the user. This will also inform the frontend of the range of the dat file id - which is used to refer to specific files. For example, a dat file id of 0 means generate heatmap for the first dat file. /by_reward/\\ /\\ /\\ url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their reward for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id. /by_episode_length/\\ /\\ /\\ url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their length for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id. /naive/ url parameters: { \"dat_id\": \"int\" } This endpoint returns the heatmap for a given dat file id. Parameters is a dat file id. /last_position/ url parameters: { \"dat_id\": \"int\" } This endpoint returns a heatmap showing the last position of every episode in a dat file. Parameters is a dat file id.","title":"Description"},{"location":"reporting_subsystem/description/#overview-of-the-reporting-subsystem","text":"","title":"Overview of the Reporting Subsystem"},{"location":"reporting_subsystem/description/#key-features","text":"","title":"Key Features"},{"location":"reporting_subsystem/description/#apis","text":"","title":"APIs"},{"location":"reporting_subsystem/description/#get","text":"","title":"GET"},{"location":"reporting_subsystem/description/#count_dat_files","text":"url parameters: {} This endpoint gets the total number of .dat files. This will inform the frontend of how many different files we have to display to the user. This will also inform the frontend of the range of the dat file id - which is used to refer to specific files. For example, a dat file id of 0 means generate heatmap for the first dat file.","title":"/count_dat_files"},{"location":"reporting_subsystem/description/#by_reward","text":"url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their reward for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id.","title":"/by_reward/\\/\\/\\"},{"location":"reporting_subsystem/description/#by_episode_length","text":"url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their length for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id.","title":"/by_episode_length/\\/\\/\\"},{"location":"reporting_subsystem/description/#naive","text":"url parameters: { \"dat_id\": \"int\" } This endpoint returns the heatmap for a given dat file id. Parameters is a dat file id.","title":"/naive/"},{"location":"reporting_subsystem/description/#last_position","text":"url parameters: { \"dat_id\": \"int\" } This endpoint returns a heatmap showing the last position of every episode in a dat file. Parameters is a dat file id.","title":"/last_position/"},{"location":"rl_subsystem/description/","text":"The main goal of the RL subsystem is to allow users to use reinforcement learning (RL) algorithms to train agents to learn to play the game. While the RL Subsystem can be used completely independently of other subsystems, the recommended use of this subsystem is to be used in conjunction with the rest of the subsystems. Overview of the RL Subsystem The RL Subsystem is made of two components: the Training Manager , and Unity's ML-Agents Python package , which is part of the ML-Agents Toolkit . ML-Agents Python Package Built by Unity , the ML-Agents Python package contains implementations of several commonly used RL algorithms, and allows agents of these algorithms to interact with the game. For more information, please refer to the official documentation of ML-Agents . Training Manager Since one of the main goals of this project is to ensure that game developers/designers can use the tool without requiring extensive knowledge in RL, the training manager is meant to further simplify the process of using the underlying ML-Agents Python package. This includes the main feature of hyperparameter tuning , which allows users to use the RL algorithms without the need to manually select the hyperparameters.","title":"Description"},{"location":"rl_subsystem/description/#overview-of-the-rl-subsystem","text":"The RL Subsystem is made of two components: the Training Manager , and Unity's ML-Agents Python package , which is part of the ML-Agents Toolkit .","title":"Overview of the RL Subsystem"},{"location":"rl_subsystem/description/#ml-agents-python-package","text":"Built by Unity , the ML-Agents Python package contains implementations of several commonly used RL algorithms, and allows agents of these algorithms to interact with the game. For more information, please refer to the official documentation of ML-Agents .","title":"ML-Agents Python Package"},{"location":"rl_subsystem/description/#training-manager","text":"Since one of the main goals of this project is to ensure that game developers/designers can use the tool without requiring extensive knowledge in RL, the training manager is meant to further simplify the process of using the underlying ML-Agents Python package. This includes the main feature of hyperparameter tuning , which allows users to use the RL algorithms without the need to manually select the hyperparameters.","title":"Training Manager"},{"location":"rl_subsystem/hyperparameter_tuning/","text":"What Are Hyperparameters? In machine learning, hyperparameters are parameters whose values are used to control the learning process. In other words, hyperparameters directly affect the result of the learned parameters. As a result, hyperparameters are not learned during the training process. Instead, hyperparameters are usually set before training, and the set of optimal hyperparameters are usually chosen empirically through a process of trial and error. This process is called hyperparameter tuning/optimization . Examples of Hyperparameters for Reinforcement Learning Some common hyperparameters include batch size, learning rate, exploration-related hyperparameters (e.g., epsilon), etc. For a more comprehensive list of hyperparameters available for various algorithms provided by Unity's ML-Agents package, please refer to here . Why Automate the Hyperparameter Tuning Process? While it is possible to manually tune hyperparameters through trial and error, this process is mundane and very expensive given the usually long training times. This process is worsen by the need for domain knowledge in order to know which hyperparameter to tune, and the scale to tune them by. Common Hyperparameter Optimization Algorithms There are a few dominant algorithms that are commonly used to automate the process of tuning the hyperparameters. This is namely Grid Search, Random/Uninformed Search, and Bayesian methods such as the Tree-structured Parzen Estimator (TPE) algorithm. REALM_AI's RL Subsystem currently supports the TPE algorithm. To get started on how to automatically tune hyperparameters with REALM_AI, head to the user guide .","title":"Hyperparameter Tuning"},{"location":"rl_subsystem/hyperparameter_tuning/#what-are-hyperparameters","text":"In machine learning, hyperparameters are parameters whose values are used to control the learning process. In other words, hyperparameters directly affect the result of the learned parameters. As a result, hyperparameters are not learned during the training process. Instead, hyperparameters are usually set before training, and the set of optimal hyperparameters are usually chosen empirically through a process of trial and error. This process is called hyperparameter tuning/optimization .","title":"What Are Hyperparameters?"},{"location":"rl_subsystem/hyperparameter_tuning/#examples-of-hyperparameters-for-reinforcement-learning","text":"Some common hyperparameters include batch size, learning rate, exploration-related hyperparameters (e.g., epsilon), etc. For a more comprehensive list of hyperparameters available for various algorithms provided by Unity's ML-Agents package, please refer to here .","title":"Examples of Hyperparameters for Reinforcement Learning"},{"location":"rl_subsystem/hyperparameter_tuning/#why-automate-the-hyperparameter-tuning-process","text":"While it is possible to manually tune hyperparameters through trial and error, this process is mundane and very expensive given the usually long training times. This process is worsen by the need for domain knowledge in order to know which hyperparameter to tune, and the scale to tune them by.","title":"Why Automate the Hyperparameter Tuning Process?"},{"location":"rl_subsystem/hyperparameter_tuning/#common-hyperparameter-optimization-algorithms","text":"There are a few dominant algorithms that are commonly used to automate the process of tuning the hyperparameters. This is namely Grid Search, Random/Uninformed Search, and Bayesian methods such as the Tree-structured Parzen Estimator (TPE) algorithm. REALM_AI's RL Subsystem currently supports the TPE algorithm. To get started on how to automatically tune hyperparameters with REALM_AI, head to the user guide .","title":"Common Hyperparameter Optimization Algorithms"},{"location":"rl_subsystem/user_guide/","text":"REALM_AI's RL hyperparameter optimization tool focuses on configurability and simplicity to use. There are only 3 steps required to use the tool: building the environment , editing the configuration file , and finally running the training procedure . Building the Environment The first step is to build the Unity ML-Agents compatible environment into an executable. ML-Agents' documentation contains a comprehensive guide that details a step-by-step guide to export the environment as an executable. Editing the Configuration File In order to make the tool as simple to use as possible, all settings reside within a single .yaml file. There are 2 main components in the configuration file -- configurations for the REALM_AI RL_Subsystem's hyperparameter optimizer, and configurations for ML-Agents. REALM_AI Configuration ML-Agents Configuration Note that we aren't doing any checking for the mlagents config portion Also note that log_unif and unif exclude upper bound Training configuration Hyperparameters Why the unconventional syntax for the hyperparameters? (To keep the yaml file concise, reduce verbosity that comes from yaml's syntax) Since Syntax Sample Configuration File Starting the Training Procedure Caveats Single behavior only The current setup assumes single-player game that contains a single behavior. Games with multiple behaviours would not work with the current script. However, due to the simplistic nature of the script, adding support for multi-agent environments is not difficult.","title":"User Guide"},{"location":"rl_subsystem/user_guide/#building-the-environment","text":"The first step is to build the Unity ML-Agents compatible environment into an executable. ML-Agents' documentation contains a comprehensive guide that details a step-by-step guide to export the environment as an executable.","title":"Building the Environment"},{"location":"rl_subsystem/user_guide/#editing-the-configuration-file","text":"In order to make the tool as simple to use as possible, all settings reside within a single .yaml file. There are 2 main components in the configuration file -- configurations for the REALM_AI RL_Subsystem's hyperparameter optimizer, and configurations for ML-Agents.","title":"Editing the Configuration File"},{"location":"rl_subsystem/user_guide/#realm_ai-configuration","text":"","title":"REALM_AI Configuration"},{"location":"rl_subsystem/user_guide/#ml-agents-configuration","text":"Note that we aren't doing any checking for the mlagents config portion Also note that log_unif and unif exclude upper bound","title":"ML-Agents Configuration"},{"location":"rl_subsystem/user_guide/#training-configuration","text":"","title":"Training configuration"},{"location":"rl_subsystem/user_guide/#hyperparameters","text":"","title":"Hyperparameters"},{"location":"rl_subsystem/user_guide/#why-the-unconventional-syntax-for-the-hyperparameters","text":"(To keep the yaml file concise, reduce verbosity that comes from yaml's syntax) Since","title":"Why the unconventional syntax for the hyperparameters?"},{"location":"rl_subsystem/user_guide/#syntax","text":"","title":"Syntax"},{"location":"rl_subsystem/user_guide/#sample-configuration-file","text":"","title":"Sample Configuration File"},{"location":"rl_subsystem/user_guide/#starting-the-training-procedure","text":"","title":"Starting the Training Procedure"},{"location":"rl_subsystem/user_guide/#caveats","text":"","title":"Caveats"},{"location":"rl_subsystem/user_guide/#single-behavior-only","text":"The current setup assumes single-player game that contains a single behavior. Games with multiple behaviours would not work with the current script. However, due to the simplistic nature of the script, adding support for multi-agent environments is not difficult.","title":"Single behavior only"}]}