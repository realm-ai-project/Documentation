{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to REALM_AI's documentation!","title":"Introduction"},{"location":"#welcome-to-realm_ais-documentation","text":"","title":"Welcome to REALM_AI's documentation!"},{"location":"installation/","text":"Installation Guide","title":"Installation"},{"location":"installation/#installation-guide","text":"","title":"Installation Guide"},{"location":"analysis_subsystem/description/","text":"The main goal of the Analysis subsystem is to automatically create different heatmaps based on the data generated upstream of the pipeline.","title":"Description"},{"location":"analysis_subsystem/heatmaps/","text":"What Are Heatmaps? Hello world Heatmap 1 - Generic Heatmap I wanna eat Heatmap 2 - I wanna sleep Heatmap 3 - I wanna cry Heatmap 4 - I wanna die","title":"Heatmaps"},{"location":"analysis_subsystem/heatmaps/#what-are-heatmaps","text":"Hello world","title":"What Are Heatmaps?"},{"location":"analysis_subsystem/heatmaps/#heatmap-1-generic-heatmap","text":"I wanna eat","title":"Heatmap 1 - Generic Heatmap"},{"location":"analysis_subsystem/heatmaps/#heatmap-2-","text":"I wanna sleep","title":"Heatmap 2 -"},{"location":"analysis_subsystem/heatmaps/#heatmap-3-","text":"I wanna cry","title":"Heatmap 3 -"},{"location":"analysis_subsystem/heatmaps/#heatmap-4-","text":"I wanna die","title":"Heatmap 4 -"},{"location":"feature_request/todo/","text":"Game Subsystem If memory becomes a problem due to size of .dat file, adjust logging frequency Reporting Subsystem Embed TensorBoard as an IFrame ( example ) within the reporting subsystem so that users do not need to go to a different website Range slider: the idea is to allow users to select the timeframe window of which they would like the heatmap to be plotted over Example: Screenshot from Plotly Related to the range slider, but a more conventional slider that allows users to view captured videos/screenshots on a timeline Example: Screenshot from Weights & Biases For the data storage file format, consider creating an EBML based format, which combines the space-efficiency of pure binary formats and the flexbility/forwards+backwards compatibility of json. The main drawbacks is that it is not easily humanly readable and may be more complex to read and write. Notable existing formats based on EBML are WEBM and MKV.","title":"Feature Request"},{"location":"feature_request/todo/#game-subsystem","text":"If memory becomes a problem due to size of .dat file, adjust logging frequency","title":"Game Subsystem"},{"location":"feature_request/todo/#reporting-subsystem","text":"Embed TensorBoard as an IFrame ( example ) within the reporting subsystem so that users do not need to go to a different website Range slider: the idea is to allow users to select the timeframe window of which they would like the heatmap to be plotted over Example: Screenshot from Plotly Related to the range slider, but a more conventional slider that allows users to view captured videos/screenshots on a timeline Example: Screenshot from Weights & Biases For the data storage file format, consider creating an EBML based format, which combines the space-efficiency of pure binary formats and the flexbility/forwards+backwards compatibility of json. The main drawbacks is that it is not easily humanly readable and may be more complex to read and write. Notable existing formats based on EBML are WEBM and MKV.","title":"Reporting Subsystem"},{"location":"game_plugin_subsystem/data_collection/","text":"coming soon... docs about data collection scripts","title":"Data Collection Components"},{"location":"game_plugin_subsystem/data_collection/#coming-soon","text":"docs about data collection scripts","title":"coming soon..."},{"location":"game_plugin_subsystem/mlagent_components/","text":"coming soon... docs about RealmAgent.cs , RealmSensorComponent.cs , RealmActuatorComponent.cs","title":"ML-Agent Components"},{"location":"game_plugin_subsystem/mlagent_components/#coming-soon","text":"docs about RealmAgent.cs , RealmSensorComponent.cs , RealmActuatorComponent.cs","title":"coming soon..."},{"location":"game_plugin_subsystem/overview/","text":"Game Plugin Subsystem Overview The game plugin subsystem is a Unity package. For more information on Unity packages, see the Unity manual or this tutorial for how to create one. Installation Users and Overview The installation process for the users is detailed in the image below, which should also provide an overview of how the package is used. For step 4, the configuration required of the user is detailed in the Prefab Breakdown page. Installation for Development For development, instead of using the \"Add package from git URL...\" option, clone the repo separately and use the \"Add package from disk...\" option. In the dialog popup, select the package.json file from the cloned repo. This will install directly from the cloned repo. You can then directly modify the package from inside Unity and the changes will be made directly to the files in the cloned repo.","title":"Overview"},{"location":"game_plugin_subsystem/overview/#game-plugin-subsystem-overview","text":"The game plugin subsystem is a Unity package. For more information on Unity packages, see the Unity manual or this tutorial for how to create one.","title":"Game Plugin Subsystem Overview"},{"location":"game_plugin_subsystem/overview/#installation-users-and-overview","text":"The installation process for the users is detailed in the image below, which should also provide an overview of how the package is used. For step 4, the configuration required of the user is detailed in the Prefab Breakdown page.","title":"Installation Users and Overview"},{"location":"game_plugin_subsystem/overview/#installation-for-development","text":"For development, instead of using the \"Add package from git URL...\" option, clone the repo separately and use the \"Add package from disk...\" option. In the dialog popup, select the package.json file from the cloned repo. This will install directly from the cloned repo. You can then directly modify the package from inside Unity and the changes will be made directly to the files in the cloned repo.","title":"Installation for Development"},{"location":"game_plugin_subsystem/prefab_breakdown/","text":"Realm AI Module Prefab Breakdown The \"Realm AI Module\" is a Unity prefab included with our package. The user should drag this into their project as a child of their player object. The following diagram is a breakdown of the different components on the prefab, as well as the setup that the user has to perform.","title":"Prefab Breakdown"},{"location":"game_plugin_subsystem/prefab_breakdown/#realm-ai-module-prefab-breakdown","text":"The \"Realm AI Module\" is a Unity prefab included with our package. The user should drag this into their project as a child of their player object. The following diagram is a breakdown of the different components on the prefab, as well as the setup that the user has to perform.","title":"Realm AI Module Prefab Breakdown"},{"location":"game_plugin_subsystem/video_recording/","text":"coming soon... docs about video recording scripts","title":"Video Recording Components"},{"location":"game_plugin_subsystem/video_recording/#coming-soon","text":"docs about video recording scripts","title":"coming soon..."},{"location":"python_gui/description/","text":"The Python GUI acts as a bridge to connect the Game Plugin Subsystem with the RL Subsystem. It provides an interface for the user to accomplish 2 tasks: Start barebones Ml-Agents Training Start Hyperparameter Tuning and use the new tuned values to start Ml-Agents Training Basic Ml-Agents Training The first use case of the Python GUI is to Hyperparameter Tuning and Effective Ml-Agent Training The second use case of the Python GUI is to","title":"Description"},{"location":"python_gui/description/#basic-ml-agents-training","text":"The first use case of the Python GUI is to","title":"Basic Ml-Agents Training"},{"location":"python_gui/description/#hyperparameter-tuning-and-effective-ml-agent-training","text":"The second use case of the Python GUI is to","title":"Hyperparameter Tuning and Effective Ml-Agent Training"},{"location":"python_gui/user_guide/","text":"hello world","title":"User Guide"},{"location":"reporting_subsystem/description/","text":"The main goal of the reporting subsystem is to be a graphical interface to view collected data on their game. Overview of the Reporting Subsystem Key Features APIs GET /count_dat_files url parameters: {} This endpoint gets the total number of .dat files. This will inform the frontend of how many different files we have to display to the user. This will also inform the frontend of the range of the dat file id - which is used to refer to specific files. For example, a dat file id of 0 means generate heatmap for the first dat file. /by_reward/\\ /\\ /\\ url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their reward for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id. /by_episode_length/\\ /\\ /\\ url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their length for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id. /naive/ url parameters: { \"dat_id\": \"int\" } This endpoint returns the heatmap for a given dat file id. Parameters is a dat file id. /last_position/ url parameters: { \"dat_id\": \"int\" } This endpoint returns a heatmap showing the last position of every episode in a dat file. Parameters is a dat file id.","title":"Description"},{"location":"reporting_subsystem/description/#overview-of-the-reporting-subsystem","text":"","title":"Overview of the Reporting Subsystem"},{"location":"reporting_subsystem/description/#key-features","text":"","title":"Key Features"},{"location":"reporting_subsystem/description/#apis","text":"","title":"APIs"},{"location":"reporting_subsystem/description/#get","text":"","title":"GET"},{"location":"reporting_subsystem/description/#count_dat_files","text":"url parameters: {} This endpoint gets the total number of .dat files. This will inform the frontend of how many different files we have to display to the user. This will also inform the frontend of the range of the dat file id - which is used to refer to specific files. For example, a dat file id of 0 means generate heatmap for the first dat file.","title":"/count_dat_files"},{"location":"reporting_subsystem/description/#by_reward","text":"url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their reward for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id.","title":"/by_reward/\\/\\/\\"},{"location":"reporting_subsystem/description/#by_episode_length","text":"url parameters: { \"type\": \"string\", \"percentage\": \"float\", \"dat_id\": \"int\" } This endpoint returns a heatmap that filters episodes by their length for a given dat file. Parameters are type (\"top\"/\"bottom\"), percentage (a float between 0 and 1), dat file id.","title":"/by_episode_length/\\/\\/\\"},{"location":"reporting_subsystem/description/#naive","text":"url parameters: { \"dat_id\": \"int\" } This endpoint returns the heatmap for a given dat file id. Parameters is a dat file id.","title":"/naive/"},{"location":"reporting_subsystem/description/#last_position","text":"url parameters: { \"dat_id\": \"int\" } This endpoint returns a heatmap showing the last position of every episode in a dat file. Parameters is a dat file id.","title":"/last_position/"},{"location":"rl_subsystem/description/","text":"The main goal of the RL subsystem is to allow users to use reinforcement learning (RL) algorithms to train agents to learn to play the game. While the RL Subsystem can be used completely independently of other subsystems, the recommended use of this subsystem is to be used in conjunction with the rest of the subsystems. Overview of the RL Subsystem The RL Subsystem is made of two components: the Training Manager , and Unity's ML-Agents Python package , which is part of the ML-Agents Toolkit . ML-Agents Python Package Built by Unity , the ML-Agents Python package contains implementations of several commonly used RL algorithms, and allows agents of these algorithms to interact with the game. For more information, please refer to the official documentation of ML-Agents . Training Manager Since one of the main goals of this project is to ensure that game developers/designers can use the tool without requiring extensive knowledge in RL, the training manager is meant to further simplify the process of using the underlying ML-Agents Python package. This includes the main feature of hyperparameter tuning , which allows users to use the RL algorithms without the need to manually select the hyperparameters.","title":"Description"},{"location":"rl_subsystem/description/#overview-of-the-rl-subsystem","text":"The RL Subsystem is made of two components: the Training Manager , and Unity's ML-Agents Python package , which is part of the ML-Agents Toolkit .","title":"Overview of the RL Subsystem"},{"location":"rl_subsystem/description/#ml-agents-python-package","text":"Built by Unity , the ML-Agents Python package contains implementations of several commonly used RL algorithms, and allows agents of these algorithms to interact with the game. For more information, please refer to the official documentation of ML-Agents .","title":"ML-Agents Python Package"},{"location":"rl_subsystem/description/#training-manager","text":"Since one of the main goals of this project is to ensure that game developers/designers can use the tool without requiring extensive knowledge in RL, the training manager is meant to further simplify the process of using the underlying ML-Agents Python package. This includes the main feature of hyperparameter tuning , which allows users to use the RL algorithms without the need to manually select the hyperparameters.","title":"Training Manager"},{"location":"rl_subsystem/hyperparameter_tuning/","text":"What Are Hyperparameters? In machine learning, hyperparameters are parameters whose values are used to control the learning process. In other words, hyperparameters directly affect the result of the learned parameters. As a result, hyperparameters are not learned during the training process. Instead, hyperparameters are usually set before training, and the set of optimal hyperparameters are usually chosen empirically through a process of trial and error. This process is called hyperparameter tuning/optimization . Examples of Hyperparameters for Reinforcement Learning Some common hyperparameters include batch size, learning rate, exploration-related hyperparameters (e.g., epsilon), etc. For a more comprehensive list of hyperparameters available for various algorithms provided by Unity's ML-Agents package, please refer to here . Why Automate the Hyperparameter Tuning Process? While it is possible to manually tune hyperparameters through trial and error, this process is mundane and very expensive given the usually long training times. This process is worsen by the need for domain knowledge in order to know which hyperparameter to tune, and the scale to tune them by. Common Hyperparameter Optimization Algorithms There are a few dominant algorithms that are commonly used to automate the process of tuning the hyperparameters. This is namely Grid Search, Random/Uninformed Search, and Bayesian methods such as the Tree-structured Parzen Estimator (TPE) algorithm. REALM_AI's RL Subsystem currently supports the TPE algorithm. To get started on how to automatically tune hyperparameters with REALM_AI, head to the user guide .","title":"Hyperparameter Tuning"},{"location":"rl_subsystem/hyperparameter_tuning/#what-are-hyperparameters","text":"In machine learning, hyperparameters are parameters whose values are used to control the learning process. In other words, hyperparameters directly affect the result of the learned parameters. As a result, hyperparameters are not learned during the training process. Instead, hyperparameters are usually set before training, and the set of optimal hyperparameters are usually chosen empirically through a process of trial and error. This process is called hyperparameter tuning/optimization .","title":"What Are Hyperparameters?"},{"location":"rl_subsystem/hyperparameter_tuning/#examples-of-hyperparameters-for-reinforcement-learning","text":"Some common hyperparameters include batch size, learning rate, exploration-related hyperparameters (e.g., epsilon), etc. For a more comprehensive list of hyperparameters available for various algorithms provided by Unity's ML-Agents package, please refer to here .","title":"Examples of Hyperparameters for Reinforcement Learning"},{"location":"rl_subsystem/hyperparameter_tuning/#why-automate-the-hyperparameter-tuning-process","text":"While it is possible to manually tune hyperparameters through trial and error, this process is mundane and very expensive given the usually long training times. This process is worsen by the need for domain knowledge in order to know which hyperparameter to tune, and the scale to tune them by.","title":"Why Automate the Hyperparameter Tuning Process?"},{"location":"rl_subsystem/hyperparameter_tuning/#common-hyperparameter-optimization-algorithms","text":"There are a few dominant algorithms that are commonly used to automate the process of tuning the hyperparameters. This is namely Grid Search, Random/Uninformed Search, and Bayesian methods such as the Tree-structured Parzen Estimator (TPE) algorithm. REALM_AI's RL Subsystem currently supports the TPE algorithm. To get started on how to automatically tune hyperparameters with REALM_AI, head to the user guide .","title":"Common Hyperparameter Optimization Algorithms"},{"location":"rl_subsystem/user_guide/","text":"REALM_AI's RL hyperparameter optimization tool focuses on configurability and simplicity to use. There are only 3 steps required to use the tool: building the environment , editing the configuration file , and finally running the training procedure . Building the Environment The first step is to build the Unity ML-Agents compatible environment into an executable. ML-Agents' documentation contains a comprehensive guide that details a step-by-step guide to export the environment as an executable. Editing the Configuration File In order to make the tool as simple to use as possible, all settings reside within a single .yaml file. There are 2 main components in the configuration file -- configurations for the REALM_AI RL_Subsystem's hyperparameter optimizer, and configurations for ML-Agents. REALM_AI Configuration ML-Agents Configuration Note that we aren't doing any checking for the mlagents config portion Also note that log_unif and unif exclude upper bound Training configuration Hyperparameters Why the unconventional syntax for the hyperparameters? (To keep the yaml file concise, reduce verbosity that comes from yaml's syntax) Since Syntax Sample Configuration File Starting the Training Procedure Caveats Single behavior only The current setup assumes single-player game that contains a single behavior. Games with multiple behaviours would not work with the current script. However, due to the simplistic nature of the script, adding support for multi-agent environments is not difficult.","title":"User Guide"},{"location":"rl_subsystem/user_guide/#building-the-environment","text":"The first step is to build the Unity ML-Agents compatible environment into an executable. ML-Agents' documentation contains a comprehensive guide that details a step-by-step guide to export the environment as an executable.","title":"Building the Environment"},{"location":"rl_subsystem/user_guide/#editing-the-configuration-file","text":"In order to make the tool as simple to use as possible, all settings reside within a single .yaml file. There are 2 main components in the configuration file -- configurations for the REALM_AI RL_Subsystem's hyperparameter optimizer, and configurations for ML-Agents.","title":"Editing the Configuration File"},{"location":"rl_subsystem/user_guide/#realm_ai-configuration","text":"","title":"REALM_AI Configuration"},{"location":"rl_subsystem/user_guide/#ml-agents-configuration","text":"Note that we aren't doing any checking for the mlagents config portion Also note that log_unif and unif exclude upper bound","title":"ML-Agents Configuration"},{"location":"rl_subsystem/user_guide/#training-configuration","text":"","title":"Training configuration"},{"location":"rl_subsystem/user_guide/#hyperparameters","text":"","title":"Hyperparameters"},{"location":"rl_subsystem/user_guide/#why-the-unconventional-syntax-for-the-hyperparameters","text":"(To keep the yaml file concise, reduce verbosity that comes from yaml's syntax) Since","title":"Why the unconventional syntax for the hyperparameters?"},{"location":"rl_subsystem/user_guide/#syntax","text":"","title":"Syntax"},{"location":"rl_subsystem/user_guide/#sample-configuration-file","text":"","title":"Sample Configuration File"},{"location":"rl_subsystem/user_guide/#starting-the-training-procedure","text":"","title":"Starting the Training Procedure"},{"location":"rl_subsystem/user_guide/#caveats","text":"","title":"Caveats"},{"location":"rl_subsystem/user_guide/#single-behavior-only","text":"The current setup assumes single-player game that contains a single behavior. Games with multiple behaviours would not work with the current script. However, due to the simplistic nature of the script, adding support for multi-agent environments is not difficult.","title":"Single behavior only"}]}